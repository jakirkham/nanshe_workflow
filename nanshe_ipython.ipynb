{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test setup. Ignore warnings during production runs.\n",
        "\n",
        "%run ./setup_tests.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Specify input data\n",
        "\n",
        "* `data_dir` (`str`): Where the data is located. (change if data is not in the current directory, normally is)\n",
        "* `data` (`str`): HDF5 file to use as input data.\n",
        "* `data_basename` (`str`): Basename to use for intermediate and final result files.\n",
        "* `dataset` (`str`): HDF5 dataset to use as input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"\"\n",
        "data = \"recording_ANM377489_20170830_1_135636.tif\"\n",
        "data_basename = \"recording_ANM377489_20170830_1_135636\"\n",
        "dataset = \"images\"\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "data_ext = os.path.splitext(data)[1].lower()\n",
        "data_dir = os.path.abspath(data_dir)\n",
        "\n",
        "subgroup_raw = \"raw\"\n",
        "subgroup_trim = \"trim\"\n",
        "subgroup_dn = \"dn\"\n",
        "subgroup_reg = \"reg\"\n",
        "subgroup_reg_images = \"reg/images\"\n",
        "subgroup_reg_shifts = \"reg/shifts\"\n",
        "subgroup_sub = \"sub\"\n",
        "subgroup_f_f0 = \"f_f0\"\n",
        "subgroup_wt = \"wt\"\n",
        "subgroup_norm = \"norm\"\n",
        "subgroup_dict = \"dict\"\n",
        "subgroup_post = \"post\"\n",
        "subgroup_post_mask = \"post/mask\"\n",
        "subgroup_rois = \"rois\"\n",
        "subgroup_rois_masks = \"rois/masks\"\n",
        "subgroup_rois_masks_j = \"rois/masks_j\"\n",
        "subgroup_rois_labels = \"rois/labels\"\n",
        "subgroup_rois_labels_j = \"rois/labels_j\"\n",
        "subgroup_traces = \"traces\"\n",
        "subgroup_proj = \"proj\"\n",
        "subgroup_proj_hmean = \"proj/hmean\"\n",
        "subgroup_proj_max = \"proj/max\"\n",
        "subgroup_proj_mean = \"proj/mean\"\n",
        "subgroup_proj_std = \"proj/std\"\n",
        "\n",
        "postfix_rois = \"_rois\"\n",
        "postfix_traces = \"_traces\"\n",
        "postfix_html = \"_proj\"\n",
        "\n",
        "h5_ext = os.path.extsep + \"h5\"\n",
        "tiff_ext = os.path.extsep + \"tif\"\n",
        "zarr_ext = os.path.extsep + \"zarr\"\n",
        "html_ext = os.path.extsep + \"html\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from psutil import cpu_count\n",
        "\n",
        "cluster_kwargs = {\n",
        "    \"preexec_commands\": (\n",
        "        \"singularity exec --bind /scratch --bind /misc ~/nanshe_nanshe_workflow_latest.img \\\\\",\n",
        "    ),\n",
        "    \"template\":\n",
        "    {\n",
        "        \"args\": [\n",
        "            \"--nthreads\", \"1\",\n",
        "            \"--local-directory\", \"/scratch/\" + os.environ[\"USER\"]\n",
        "        ],\n",
        "        \"jobEnvironment\": os.environ\n",
        "    }\n",
        "}\n",
        "client_kwargs = {}\n",
        "adaptive_kwargs = {\n",
        "    \"startup_cost\": \"12s\",\n",
        "    \"interval\": \"6s\",\n",
        "    \"minimum\": 0,\n",
        "    \"maximum\": 32,\n",
        "#     \"maximum\": int(os.environ.get(\"CORES\", cpu_count())) - 1\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zarr\n",
        "\n",
        "from nanshe_workflow.data import DistributedDirectoryStore\n",
        "\n",
        "zarr_store = zarr.open_group(DistributedDirectoryStore(data_basename + zarr_ext), \"a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure and startup Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanshe_workflow.par import startup_distributed\n",
        "from nanshe_workflow.data import DistributedArrayStore\n",
        "\n",
        "client = startup_distributed(0, cluster_kwargs, client_kwargs, adaptive_kwargs)\n",
        "\n",
        "dask_store = DistributedArrayStore(zarr_store, client=client)\n",
        "\n",
        "client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define functions for computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.cm\n",
        "import matplotlib.pyplot\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mplview.core import MatplotlibViewer as MPLViewer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from builtins import range as irange\n",
        "\n",
        "try:\n",
        "    from contextlib import suppress\n",
        "except ImportError:\n",
        "    from contextlib2 import suppress\n",
        "\n",
        "import numpy\n",
        "import scipy\n",
        "import scipy.ndimage\n",
        "import h5py\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.ndimage as spim\n",
        "import h5py as hp\n",
        "\n",
        "import dask\n",
        "import dask.array\n",
        "import dask.array.fft\n",
        "import dask.distributed\n",
        "\n",
        "import dask.array as da\n",
        "\n",
        "import dask_imread\n",
        "import dask_ndfilters\n",
        "import dask_ndfourier\n",
        "\n",
        "import zarr\n",
        "\n",
        "import nanshe\n",
        "from nanshe.imp.segment import generate_dictionary\n",
        "\n",
        "import nanshe_workflow\n",
        "from nanshe_workflow.data import io_remove, dask_io_remove, dask_load_hdf5, dask_store_zarr, zip_zarr, open_zarr\n",
        "\n",
        "zarr.blosc.set_nthreads(1)\n",
        "zarr.blosc.use_threads = False\n",
        "client.run(zarr.blosc.set_nthreads, 1)\n",
        "client.run(setattr, zarr.blosc, \"use_threads\", False)\n",
        "\n",
        "logging.getLogger(\"nanshe\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanshe_workflow.data import DistributedDirectoryStore\n",
        "from nanshe_workflow.data import hdf5_to_zarr, zarr_to_hdf5\n",
        "from nanshe_workflow.data import save_tiff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import pyfftw.interfaces.numpy_fft as numpy_fft\n",
        "except ImportError:\n",
        "    import numpy.fft as numpy_fft\n",
        "\n",
        "rfftn = da.fft.fft_wrap(numpy_fft.rfftn)\n",
        "irfftn = da.fft.fft_wrap(numpy_fft.irfftn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanshe_workflow.imp2 import extract_f0, wavelet_transform, renormalized_images, normalize_data\n",
        "\n",
        "from nanshe_workflow.par import halo_block_generate_dictionary_parallel\n",
        "from nanshe_workflow.imp import block_postprocess_data_parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanshe_workflow.proj2 import compute_traces\n",
        "\n",
        "from nanshe_workflow.proj2 import compute_adj_harmonic_mean_projection\n",
        "\n",
        "from nanshe_workflow.proj2 import norm_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zarr_store.get(subgroup_proj, {}).keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Begin workflow. Set parameters and run each cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert TIFF/HDF5 to Zarr\n",
        "\n",
        "* `block_chunks` (`tuple` of `int`s): chunk size for each block loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "block_chunks = (100, -1, -1)\n",
        "\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_raw]\n",
        "\n",
        "if data_ext == tiff_ext:\n",
        "    dask_store[subgroup_raw] = dask_imread.imread(data, nframes=block_chunks[0])\n",
        "elif data_ext == h5_ext:\n",
        "    dask_store[subgroup_raw] = dask_load_hdf5(data, dataset, chunks=block_chunks)\n",
        "\n",
        "dask.distributed.progress(dask_store[subgroup_raw], notebook=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Input Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imgs_min, imgs_max = 0, 100\n",
        "\n",
        "da_imgs = dask_store[subgroup_raw]\n",
        "\n",
        "da_imgs_min, da_imgs_max = da_imgs.min(), da_imgs.max()\n",
        "\n",
        "status = client.compute([da_imgs_min, da_imgs_max])\n",
        "dask.distributed.progress(status, notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "imgs_min, imgs_max = [s.result() for s in status]\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    da_imgs,\n",
        "    vmin=imgs_min,\n",
        "    vmax=imgs_max\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trimming\n",
        "\n",
        "* `front` (`int`): amount to trim off the front\n",
        "* `back` (`int`): amount to trim off the back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "front = 1\n",
        "back = 1\n",
        "\n",
        "\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_trim]\n",
        "\n",
        "\n",
        "# Load and prep data for computation.\n",
        "da_imgs = dask_store[subgroup_raw]\n",
        "\n",
        "# Trim frames from front and back\n",
        "da_imgs_trim = da_imgs[front:len(da_imgs)-back]\n",
        "\n",
        "# Store trimmed data\n",
        "dask_store[subgroup_trim] = da_imgs_trim\n",
        "\n",
        "# Check progress of store step\n",
        "dask.distributed.progress(dask_store[subgroup_trim], notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# View results\n",
        "imgs_min, imgs_max = 0, 100\n",
        "\n",
        "da_imgs = dask_store[subgroup_trim]\n",
        "\n",
        "da_imgs_min, da_imgs_max = da_imgs.min(), da_imgs.max()\n",
        "\n",
        "status = client.compute([da_imgs_min, da_imgs_max])\n",
        "dask.distributed.progress(status, notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "imgs_min, imgs_max = [s.result() for s in status]\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    da_imgs,\n",
        "    vmin=imgs_min,\n",
        "    vmax=imgs_max\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Registration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fourier_shift_wrap(array, shift):\n",
        "    result = numpy.empty_like(array)\n",
        "    for i in irange(len(array)):\n",
        "        result[i] = spim.fourier_shift(array[i], shift[0][i])\n",
        "    return result\n",
        "\n",
        "\n",
        "def find_best_match(matches):\n",
        "    best_match = numpy.zeros(\n",
        "        matches.shape[:1],\n",
        "        dtype=matches.dtype\n",
        "    )\n",
        "    if matches.size:\n",
        "        i = numpy.argmin((matches ** 2).sum(axis=0))\n",
        "        best_match = matches[:, i]\n",
        "\n",
        "    return best_match\n",
        "\n",
        "\n",
        "def compute_offset(match_mask):\n",
        "    match_mask = match_mask[0][0]\n",
        "\n",
        "    result = numpy.empty((len(match_mask), match_mask.ndim - 1), dtype=int)\n",
        "    for i in irange(len(match_mask)):\n",
        "        match_mask_i = match_mask[i]\n",
        "\n",
        "        frame_shape = np.array(match_mask_i.shape)\n",
        "        half_frame_shape = frame_shape // 2\n",
        "\n",
        "        matches = np.array(match_mask_i.nonzero())\n",
        "        above = (matches > half_frame_shape[:, None]).astype(matches.dtype)\n",
        "        matches -= above * frame_shape[:, None]\n",
        "\n",
        "        result[i] = find_best_match(matches)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_reps = 3\n",
        "tmpl_hist_wght = 0.25\n",
        "thld_rel_dist = 0.0\n",
        "\n",
        "\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_reg]\n",
        "zarr_store.require_group(subgroup_reg)\n",
        "\n",
        "\n",
        "# Load and prep data for computation.\n",
        "da_imgs = dask_store[subgroup_trim]\n",
        "\n",
        "da_imgs_flt = da_imgs\n",
        "if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "        da_imgs_flt.dtype.itemsize >= 4):\n",
        "    da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "# Create frame shape arrays\n",
        "frame_shape = np.array(da_imgs_flt.shape[1:], dtype=int)\n",
        "half_frame_shape = frame_shape // 2\n",
        "frame_shape = da.asarray(frame_shape)\n",
        "half_frame_shape = da.asarray(half_frame_shape)\n",
        "\n",
        "# Find the inverse of each frame\n",
        "da_imgs_flt_min = da_imgs_flt.min()\n",
        "da_imgs_inv = dask.array.reciprocal(da_imgs_flt - (da_imgs_flt_min - 1))\n",
        "\n",
        "# Compute the FFT of inverse frames and template\n",
        "da_imgs_fft = rfftn(da_imgs_inv, axes=tuple(irange(1, da_imgs_flt.ndim)))\n",
        "da_imgs_fft_tmplt = da_imgs_fft.mean(axis=0, keepdims=True)\n",
        "\n",
        "# Initialize\n",
        "i = 0\n",
        "avg_rel_dist = 1.0\n",
        "tmpl_hist_wght = da_imgs_flt.dtype.type(tmpl_hist_wght)\n",
        "da_shifts = da.zeros(\n",
        "    (len(da_imgs_flt), da_imgs_flt.ndim - 1),\n",
        "    dtype=int,\n",
        "    chunks=(1, da_imgs_flt.ndim - 1)\n",
        ")\n",
        "\n",
        "# Persist FFT of frames and template\n",
        "da_imgs_flt_min, da_imgs_fft, da_imgs_fft_tmplt = client.persist([\n",
        "    da_imgs_flt_min, da_imgs_fft, da_imgs_fft_tmplt\n",
        "])\n",
        "dask.distributed.fire_and_forget(da_imgs_flt_min)\n",
        "del da_imgs_flt_min\n",
        "\n",
        "while avg_rel_dist > thld_rel_dist and i < num_reps:\n",
        "    # Compute the shifted frames\n",
        "    da_shifted_frames = da.atop(\n",
        "        fourier_shift_wrap,\n",
        "        (0,) + tuple(irange(1, da_imgs_fft.ndim)),\n",
        "        da_imgs_fft,\n",
        "        (0,) + tuple(irange(1, da_imgs_fft.ndim)),\n",
        "        da_shifts,\n",
        "        (0, da_imgs_fft.ndim),\n",
        "        dtype=da_imgs_fft.dtype\n",
        "    )\n",
        "\n",
        "    # Compute the template FFT\n",
        "    da_imgs_fft_tmplt = (\n",
        "        tmpl_hist_wght * da_imgs_fft_tmplt +\n",
        "        (1 - tmpl_hist_wght) * da_shifted_frames.mean(axis=0, keepdims=True)\n",
        "    )\n",
        "\n",
        "    # Free connected persisted values\n",
        "    del da_shifted_frames\n",
        "\n",
        "    # Find the best overlap with the template.\n",
        "    da_overlap = irfftn(\n",
        "        da_imgs_fft * da_imgs_fft_tmplt,\n",
        "        s=da_imgs_flt.shape[1:],\n",
        "        axes=tuple(irange(1, da_imgs_flt.ndim))\n",
        "    )\n",
        "    da_overlap_max = da_overlap.max(axis=tuple(irange(1, da_imgs_flt.ndim)), keepdims=True)\n",
        "    da_overlap_max_match = (da_overlap == da_overlap_max)\n",
        "\n",
        "    # Compute the shift for each frame.\n",
        "    old_da_shifts = da_shifts\n",
        "    da_shifts = da.atop(\n",
        "        compute_offset,\n",
        "        (0, da_overlap_max_match.ndim),\n",
        "        da_overlap_max_match.rechunk(dict(enumerate(da_overlap_max_match.shape[1:], 1))),\n",
        "        tuple(irange(0, da_overlap_max_match.ndim)),\n",
        "        dtype=int,\n",
        "        new_axes={da_overlap_max_match.ndim: da_overlap_max_match.ndim - 1}\n",
        "    )\n",
        "\n",
        "    # Free connected persisted values\n",
        "    del da_overlap\n",
        "    del da_overlap_max\n",
        "    del da_overlap_max_match\n",
        "\n",
        "    # Remove any collective frame drift.\n",
        "    da_drift = da_shifts.mean(axis=0, keepdims=True).round().astype(da_shifts.dtype)\n",
        "    da_shifts = da_shifts - da_drift\n",
        "\n",
        "    # Free connected persisted values\n",
        "    del da_drift\n",
        "\n",
        "    # Find shift change.\n",
        "    diff_da_shifts = da_shifts - old_da_shifts\n",
        "    rel_diff_da_shifts = (\n",
        "        diff_da_shifts.astype(da_imgs_flt.dtype) / \n",
        "        frame_shape.astype(da_imgs_flt.dtype) /\n",
        "        (da_imgs_flt.dtype.type(len(frame_shape)) ** 0.5)\n",
        "    )\n",
        "    rel_dist_da_shifts = (rel_diff_da_shifts ** 2.0).sum(axis=1) ** 0.5\n",
        "    avg_rel_dist = rel_dist_da_shifts.sum() / da_imgs_flt.dtype.type(len(da_shifts))\n",
        "\n",
        "    # Free old shifts\n",
        "    del old_da_shifts\n",
        "\n",
        "    # Free connected persisted values\n",
        "    del diff_da_shifts\n",
        "    del rel_diff_da_shifts\n",
        "    del rel_dist_da_shifts\n",
        "\n",
        "    # Persist values needed for the next iteration (and end of this one).\n",
        "    da_imgs_fft_tmplt, da_shifts, avg_rel_dist = client.persist([da_imgs_fft_tmplt, da_shifts, avg_rel_dist])\n",
        "    dask.distributed.fire_and_forget(da_shifts)\n",
        "\n",
        "    # Compute change\n",
        "    dask.distributed.progress(avg_rel_dist, notebook=False)\n",
        "    print(\"\")\n",
        "    avg_rel_dist = avg_rel_dist.compute()\n",
        "    i += 1\n",
        "\n",
        "    # Show change\n",
        "    print((i, avg_rel_dist))\n",
        "\n",
        "# Drop unneeded items\n",
        "del frame_shape\n",
        "del half_frame_shape\n",
        "del da_imgs_flt\n",
        "del da_imgs_inv\n",
        "del da_imgs_fft\n",
        "del da_imgs_fft_tmplt\n",
        "\n",
        "# Truncate shifted part of each frame\n",
        "da_imgs_trunc = []\n",
        "da_imgs_trunc_shape = da_imgs.shape[1:]\n",
        "for i in irange(len(da_imgs)):\n",
        "    slice_i = [i]\n",
        "    shift_i = numpy.asarray(da_shifts[i])[()]\n",
        "    for j in irange(len(shift_i)):\n",
        "        shifts_ij = shift_i[j]\n",
        "        if shifts_ij < 0:\n",
        "            slice_i.append(slice(-shifts_ij, None))\n",
        "        elif shifts_ij > 0:\n",
        "            slice_i.append(slice(None, -shifts_ij))\n",
        "        else:\n",
        "            slice_i.append(slice(None))\n",
        "    slice_i = tuple(slice_i)\n",
        "    da_imgs_trunc.append(da_imgs[slice_i])\n",
        "    da_imgs_trunc_shape = tuple(np.minimum(\n",
        "        da_imgs_trunc_shape, da_imgs_trunc[-1].shape\n",
        "    ))\n",
        "\n",
        "# Free raw data\n",
        "del da_imgs\n",
        "\n",
        "# Truncate all frames to smallest one\n",
        "da_imgs_trunc_cut = tuple(map(\n",
        "    lambda s: slice(None, s), da_imgs_trunc_shape\n",
        "))\n",
        "for i in irange(len(da_imgs_trunc)):\n",
        "    da_imgs_trunc[i] = da_imgs_trunc[i][da_imgs_trunc_cut]\n",
        "da_imgs_trunc = da.stack(da_imgs_trunc)\n",
        "\n",
        "# Store registered data\n",
        "dask_store.update({\n",
        "    subgroup_reg_images: da_imgs_trunc,\n",
        "    subgroup_reg_shifts: da_shifts,\n",
        "})\n",
        "\n",
        "dask.distributed.progress(\n",
        "    dask.distributed.futures_of([\n",
        "        dask_store[subgroup_reg_images],\n",
        "        dask_store[subgroup_reg_shifts]\n",
        "    ]),\n",
        "    notebook=False\n",
        ")\n",
        "print(\"\")\n",
        "\n",
        "# Free truncated frames and shifts\n",
        "del da_imgs_trunc\n",
        "del da_shifts\n",
        "\n",
        "\n",
        "# View results\n",
        "imgs_min, imgs_max = 0, 100\n",
        "\n",
        "da_imgs = dask_store[subgroup_reg_images]\n",
        "da_shifts = dask_store[subgroup_reg_shifts]\n",
        "\n",
        "da_imgs_min, da_imgs_max = da_imgs.min(), da_imgs.max()\n",
        "\n",
        "status = client.compute([da_imgs_min, da_imgs_max])\n",
        "dask.distributed.progress(status, notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "imgs_min, imgs_max = [s.result() for s in status]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=da_shifts.shape[1], sharex=True)\n",
        "fig.subplots_adjust(hspace=0.0)\n",
        "for i in range(da_shifts.shape[1]):\n",
        "    axs[i].plot(np.asarray(da_shifts[:, i]))\n",
        "    axs[i].set_ylabel(\"%s (px)\" % chr(ord(\"X\") + da_shifts.shape[1] - i - 1))\n",
        "    axs[i].yaxis.set_tick_params(width=1.5)\n",
        "    [v.set_linewidth(2) for v in axs[i].spines.values()]\n",
        "axs[-1].set_xlabel(\"Frame (#)\")\n",
        "axs[-1].set_xlim((0, da_shifts.shape[0] - 1))\n",
        "axs[-1].xaxis.set_tick_params(width=1.5)\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    da_imgs,\n",
        "    vmin=imgs_min,\n",
        "    vmax=imgs_max\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Denoising\n",
        "\n",
        "* `med_filt_size` (`int`): footprint size for median filter\n",
        "* `norm_filt_sigma` (`int`/`float`): sigma for Gaussian filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "med_filt_size = 5\n",
        "norm_filt_sigma = 10\n",
        "\n",
        "\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_dn]\n",
        "\n",
        "\n",
        "# Load and prep data for computation.\n",
        "da_imgs = dask_store[subgroup_reg_images]\n",
        "\n",
        "da_imgs_flt = da_imgs\n",
        "if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "        da_imgs_flt.dtype.itemsize >= 4):\n",
        "    da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "# Median filter frames\n",
        "da_imgs_medf = dask_ndfilters.median_filter(\n",
        "    da_imgs_flt, (1,) + (da_imgs_flt.ndim - 1) * (med_filt_size,)\n",
        ")\n",
        "\n",
        "# Compute the Gaussian filter of frames\n",
        "da_imgs_smoothed = dask_ndfilters.gaussian_filter(\n",
        "    da_imgs_medf, (0,) + (da_imgs_medf.ndim - 1) * (norm_filt_sigma,)\n",
        ")\n",
        "\n",
        "# Apply high pass filter to images\n",
        "da_imgs_filt = da_imgs_medf - da_imgs_smoothed\n",
        "\n",
        "# Reset minimum to original value.\n",
        "da_imgs_filt += da_imgs.min() - da_imgs_filt.min()\n",
        "\n",
        "# Store denoised data\n",
        "dask_store[subgroup_dn] = da_imgs_filt\n",
        "\n",
        "# Check progress of store step\n",
        "dask.distributed.progress(dask_store[subgroup_dn])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.cluster.scale_up(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check progress of store step\n",
        "dask.distributed.progress(dask_store[subgroup_dn], notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# View results\n",
        "imgs_min, imgs_max = 0, 100\n",
        "\n",
        "da_imgs = dask_store[subgroup_dn]\n",
        "\n",
        "da_imgs_min, da_imgs_max = da_imgs.min(), da_imgs.max()\n",
        "\n",
        "status = client.compute([da_imgs_min, da_imgs_max])\n",
        "dask.distributed.progress(status, notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "imgs_min, imgs_max = [s.result() for s in status]\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    da_imgs,\n",
        "    vmin=imgs_min,\n",
        "    vmax=imgs_max\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Projections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_proj]\n",
        "zarr_store.require_group(subgroup_proj)\n",
        "\n",
        "\n",
        "# Load and prep data for computation.\n",
        "da_imgs = dask_store[subgroup_dn]\n",
        "\n",
        "da_imgs_flt = da_imgs\n",
        "if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "        da_imgs_flt.dtype.itemsize >= 4):\n",
        "    da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "da_imgs_proj_hmean = compute_adj_harmonic_mean_projection(da_imgs_flt)\n",
        "\n",
        "da_imgs_proj_max = da_imgs_flt.max(axis=0)\n",
        "\n",
        "da_imgs_proj_mean, da_imgs_proj_std = da_imgs_flt.mean(axis=0), da_imgs_flt.std(axis=0)\n",
        "\n",
        "# Store projections\n",
        "dask_store.update(dict(zip(\n",
        "    [subgroup_proj_hmean, subgroup_proj_max, subgroup_proj_mean, subgroup_proj_std],\n",
        "    [da_imgs_proj_hmean, da_imgs_proj_max, da_imgs_proj_mean, da_imgs_proj_std]\n",
        ")))\n",
        "\n",
        "dask.distributed.progress(\n",
        "    dask.distributed.futures_of([\n",
        "        dask_store[subgroup_proj_hmean],\n",
        "        dask_store[subgroup_proj_max],\n",
        "        dask_store[subgroup_proj_mean],\n",
        "        dask_store[subgroup_proj_std]\n",
        "    ]),\n",
        "    notebook=False\n",
        ")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subtract Projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_sub]\n",
        "\n",
        "\n",
        "# Load and prep data for computation.\n",
        "da_imgs = dask_store[subgroup_dn]\n",
        "\n",
        "da_imgs_flt = da_imgs\n",
        "if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "        da_imgs_flt.dtype.itemsize >= 4):\n",
        "    da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "da_imgs_sub = da_imgs_flt - compute_adj_harmonic_mean_projection(da_imgs_flt)\n",
        "da_imgs_sub -= da_imgs_sub.min()\n",
        "\n",
        "# Store background removed data\n",
        "dask_store[subgroup_sub] = da_imgs_sub\n",
        "\n",
        "dask.distributed.progress(dask_store[subgroup_sub], notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# View results\n",
        "imgs_min, imgs_max = 0, 100\n",
        "\n",
        "da_imgs = dask_store[subgroup_sub]\n",
        "\n",
        "da_imgs_min, da_imgs_max = da_imgs.min(), da_imgs.max()\n",
        "\n",
        "status = client.compute([da_imgs_min, da_imgs_max])\n",
        "dask.distributed.progress(status, notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "imgs_min, imgs_max = [s.result() for s in status]\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    da_imgs,\n",
        "    vmin=imgs_min,\n",
        "    vmax=imgs_max\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "da_imgs_edges[0].min().compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "da_imgs_smooth[0].max().compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.cluster.scale_up(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "da_imgs_smooth.max(axis=0).compute().min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    (da_imgs_smooth.max(axis=0) > 0).astype(np.uint8).compute(),\n",
        "    vmin=0,\n",
        "    vmax=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "da_imgs = dask_store[subgroup_sub]\n",
        "da_imgs = da_imgs.rechunk({i: s for i, s in enumerate(da_imgs.shape[1:], 1)})\n",
        "\n",
        "da_imgs_smooth = da_imgs.map_blocks(\n",
        "    scipy.ndimage.spline_filter,\n",
        "    dtype=da_imgs.dtype,\n",
        "    order=5\n",
        ")\n",
        "\n",
        "da_imgs_smooth = da_imgs_smooth.map_blocks(\n",
        "    scipy.ndimage.median_filter,\n",
        "    dtype=da_imgs.dtype,\n",
        "    size=5\n",
        ")\n",
        "\n",
        "da_imgs_smooth = da_imgs_smooth.persist()\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    da_imgs_smooth.max(axis=0).persist(),\n",
        "    vmin=-200,\n",
        "    vmax=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "da_imgs = dask_store[subgroup_sub]\n",
        "da_imgs = da_imgs.rechunk({i: s for i, s in enumerate(da_imgs.shape[1:], 1)})\n",
        "\n",
        "da_imgs_edges = da_imgs.map_blocks(\n",
        "    scipy.ndimage.filters.gaussian_laplace,\n",
        "    dtype=da_imgs.dtype,\n",
        "    sigma=10.0\n",
        ")\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    da_imgs_edges,\n",
        "    vmin=imgs_min,\n",
        "    vmax=imgs_max\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_norm]\n",
        "\n",
        "\n",
        "# Load and prep data for computation.\n",
        "da_imgs = dask_store[subgroup_sub]\n",
        "\n",
        "da_imgs_flt = da_imgs\n",
        "if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "        da_imgs_flt.dtype.itemsize >= 4):\n",
        "    da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "da_imgs_flt_mins = da_imgs_flt.min(\n",
        "    axis=tuple(irange(1, da_imgs_flt.ndim)),\n",
        "    keepdims=True\n",
        ")\n",
        "\n",
        "da_imgs_flt_shift = da_imgs_flt - da_imgs_flt_mins\n",
        "\n",
        "da_result = renormalized_images(da_imgs_flt_shift)\n",
        "\n",
        "# Store normalized data\n",
        "dask_store[subgroup_norm] = da_result\n",
        "\n",
        "dask.distributed.progress(dask_store[subgroup_norm], notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# View results\n",
        "imgs_min, imgs_max = 0, 100\n",
        "\n",
        "da_imgs = dask_store[subgroup_norm]\n",
        "\n",
        "da_imgs_min, da_imgs_max = da_imgs.min(), da_imgs.max()\n",
        "\n",
        "status = client.compute([da_imgs_min, da_imgs_max])\n",
        "dask.distributed.progress(status, notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "imgs_min, imgs_max = [s.result() for s in status]\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    da_imgs,\n",
        "    vmin=imgs_min,\n",
        "    vmax=imgs_max\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dictionary Learning\n",
        "\n",
        "* `n_components` (`int`): number of basis images in the dictionary.\n",
        "* `batchsize` (`int`): minibatch size to use.\n",
        "* `iters` (`int`): number of iterations to run before getting dictionary.\n",
        "* `lambda1` (`float`): weight for L<sup>1</sup> sparisty enforcement on sparse code.\n",
        "* `lambda2` (`float`): weight for L<sup>2</sup> sparisty enforcement on sparse code.\n",
        "\n",
        "<br>\n",
        "* `block_frames` (`int`): number of frames to work with in each full frame block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_components = 50\n",
        "batchsize = 256\n",
        "iters = 100\n",
        "lambda1 = 0.2\n",
        "lambda2 = 0.0\n",
        "\n",
        "block_frames = 51\n",
        "\n",
        "\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_dict]\n",
        "\n",
        "\n",
        "imgs = dask_store._diskstore[subgroup_norm]\n",
        "\n",
        "block_shape = (block_frames,) + imgs.shape[1:]\n",
        "da_imgs = da.from_array(imgs, chunks=block_shape)\n",
        "\n",
        "new_result = dask_store._create_dataset(\n",
        "    subgroup_dict, shape=(n_components,) + da_imgs.shape[1:], dtype=da_imgs.dtype, chunks=True\n",
        ")\n",
        "halo_block_generate_dictionary_parallel(client, None)(generate_dictionary)(block_shape)(\n",
        "    da_imgs,\n",
        "    n_components=n_components,\n",
        "    out=new_result,\n",
        "    **{\"sklearn.decomposition.dict_learning_online\" : {\n",
        "            \"n_jobs\" : 1,\n",
        "            \"n_iter\" : iters,\n",
        "            \"batch_size\" : batchsize,\n",
        "            \"alpha\" : lambda1\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "dask.distributed.progress(dask_store[subgroup_dict], notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# View results\n",
        "imgs_min, imgs_max = 0, 100\n",
        "\n",
        "da_imgs = dask_store[subgroup_dict]\n",
        "\n",
        "da_imgs_min, da_imgs_max = da_imgs.min(), da_imgs.max()\n",
        "\n",
        "status = client.compute([da_imgs_min, da_imgs_max])\n",
        "dask.distributed.progress(status, notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "imgs_min, imgs_max = [s.result() for s in status]\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    da_imgs,\n",
        "    vmin=imgs_min,\n",
        "    vmax=imgs_max\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Postprocessing\n",
        "\n",
        "* `significance_threshold` (`float`): number of standard deviations below which to include in \"noise\" estimate\n",
        "* `noise_threshold` (`float`): number of units of \"noise\" above which something needs to be to be significant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(\n",
        "    np.asarray(\n",
        "        dask_store[subgroup_norm].mean(axis=tuple(range(1, dask_store[subgroup_norm].ndim))) /\n",
        "        dask_store[subgroup_norm].std(axis=tuple(range(1, dask_store[subgroup_norm].ndim)))\n",
        "    ),\n",
        "    label=\"snr\"\n",
        ")\n",
        "plt.plot(\n",
        "    np.asarray(dask_store[subgroup_norm].max(axis=tuple(range(1, dask_store[subgroup_norm].ndim)))),\n",
        "    label=\"max\"\n",
        ")\n",
        "plt.plot(\n",
        "    np.asarray(dask_store[subgroup_norm].min(axis=tuple(range(1, dask_store[subgroup_norm].ndim)))),\n",
        "    label=\"min\"\n",
        ")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with suppress(KeyError):\n",
        "    del dask_store[\"snr_sel\"]\n",
        "\n",
        "\n",
        "da_imgs = dask_store[subgroup_norm]\n",
        "\n",
        "da_imgs_snr = (\n",
        "    da_imgs.mean(axis=tuple(range(1, da_imgs.ndim))) /\n",
        "    da_imgs.std()\n",
        "#     da_imgs.std(axis=tuple(range(1, da_imgs.ndim)))\n",
        ")\n",
        "\n",
        "da_result = da_imgs[da_imgs_snr > 2.0, :, :]\n",
        "\n",
        "da_imgs_snr, da_result = client.persist([da_imgs_snr, da_result])\n",
        "dask.distributed.fire_and_forget([da_imgs_snr])\n",
        "del da_imgs_snr\n",
        "\n",
        "dask.distributed.progress(da_result, notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "# Make chunks concrete\n",
        "\n",
        "# Find chunk size\n",
        "da_result_chunks_0 = tuple(\n",
        "    da_result[:, 0, 0].map_blocks(lambda e: np.atleast_1d(np.ones_like(e).astype(int).sum())).compute()\n",
        ")\n",
        "da_result_chunks = (\n",
        "    (da_result_chunks_0,) + da_result.chunks[1:]\n",
        ")\n",
        "\n",
        "# Drop out empty chunks\n",
        "da_result_chunks_0 = tuple(\n",
        "    da_result[:, 0, 0].map_blocks(lambda e: np.atleast_1d(e.shape[0])).compute()\n",
        ")\n",
        "da_result_keys_0, da_result_chunks_0 = list(zip(*[\n",
        "    [k, c] for k, c in zip(dask.core.flatten(da_result.__dask_keys__()), da_result_chunks_0) if c\n",
        "]))\n",
        "da_result_keys_0 = list(da_result_keys_0)\n",
        "da_result_chunks = (\n",
        "    (da_result_chunks_0,) + da_result.chunks[1:]\n",
        ")\n",
        "\n",
        "da_result_2_name = \"concrete-\" + da_result.name\n",
        "da_result_2_dsk = dask.sharedict.merge(\n",
        "    da_result.dask,\n",
        "    (\n",
        "        da_result_2_name,\n",
        "        {(da_result_2_name, i) + k[2:]: k for i, k in enumerate(da_result_keys_0)}\n",
        "    )\n",
        ")\n",
        "\n",
        "# Rebuild Array as concrete\n",
        "da_result_2 = da.Array(\n",
        "    da_result_2_dsk,\n",
        "    da_result_2_name,\n",
        "    da_result_chunks,\n",
        "    da_result.dtype\n",
        ")\n",
        "\n",
        "dask_store[\"snr_sel\"] = da_result_2\n",
        "\n",
        "del da_result\n",
        "del da_result_2\n",
        "\n",
        "dask.distributed.progress(dask_store[\"snr_sel\"], notebook=False)\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imgs_min, imgs_max = da.compute(\n",
        "    dask_store[\"snr_sel\"].min(), dask_store[\"snr_sel\"].max()\n",
        ")\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    dask_store[\"snr_sel\"],\n",
        "    vmin=imgs_min,\n",
        "    vmax=imgs_max\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def np_label(a):\n",
        "    return np.array(\n",
        "        scipy.ndimage.label(a),\n",
        "        dtype=[\n",
        "            (\"label\", np.int32, da_imgs_thrd.shape),\n",
        "            (\"num\", int, ()),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def label_chunk(a):\n",
        "    return np.stack([np_label(e) for e in a])\n",
        "\n",
        "def label(d):\n",
        "    d_lbld = d.map_blocks(\n",
        "        label_chunk,\n",
        "        dtype=[\n",
        "            (\"label\", np.int32, d.shape[1:]),\n",
        "            (\"num\", int, ()),\n",
        "        ],\n",
        "        drop_axis=tuple(irange(1, d.ndim))\n",
        "    )\n",
        "\n",
        "    return d_lbld[\"label\"], d_lbld[\"num\"]\n",
        "\n",
        "def np_labels_to_masks_chunk(a, num):\n",
        "    r = np.empty((0,) + a.shape[1:], dtype=bool)\n",
        "    if num:\n",
        "        r = np.concatenate([a == i for i in irange(1, 1 + num)])\n",
        "    return r\n",
        "\n",
        "def labels_to_masks_chunk(a, num):\n",
        "    r = np.empty((0,) + a.shape[2:], dtype=bool)\n",
        "    if len(num):\n",
        "        r = np.concatenate([np_labels_to_masks_chunk(a[i], num[i]) for i in range(len(a))])\n",
        "#         r = np.concatenate([np_labels_to_masks_chunk(e0, e1) for e0, e1 in zip(a, num)])\n",
        "    return r\n",
        "\n",
        "def labels_to_masks(d, nums):\n",
        "    out = da.atop(\n",
        "        labels_to_masks_chunk, tuple(irange(d.ndim)),\n",
        "        d, tuple(irange(d.ndim)),\n",
        "        nums, tuple(irange(nums.ndim)),\n",
        "        dtype=bool\n",
        "    )\n",
        "    out._chunks = (len(out.chunks[0]) * (np.nan,),) + out.chunks[1:]\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dask_ndmorph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "percentile_threshold = 0.7\n",
        "\n",
        "with suppress(KeyError):\n",
        "    del dask_store[\"thrsh_post\"]\n",
        "\n",
        "\n",
        "# Find masks for each dictionary image\n",
        "\n",
        "da_imgs = dask_store[\"snr_sel\"]\n",
        "da_imgs = da_imgs.rechunk(((1,) + da_imgs.shape[1:]))\n",
        "\n",
        "da_imgs_snr = da_imgs.map_blocks(\n",
        "    np.percentile,\n",
        "    dtype=da_imgs_snr.dtype,\n",
        "    drop_axis=tuple(range(1, da_imgs.ndim)),\n",
        "    new_axis=tuple(range(1, da_imgs.ndim)),\n",
        "    chunks=(da_imgs_snr.chunks[0],) + (da_imgs_snr.ndim - 1) * (1,),\n",
        "    q=100.0 * percentile_threshold,\n",
        "    keepdims=True\n",
        ")\n",
        "da_imgs_snr = da_imgs_snr.persist()\n",
        "\n",
        "da_imgs_thrd = (da_imgs > da_imgs_snr).astype(np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "dask_store[\"thrd_post\"] = da_imgs_thrd\n",
        "\n",
        "\n",
        "dask.distributed.progress(dask_store[\"thrd_post\"], notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# View results\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    dask_store[\"thrd_post\"],\n",
        "    vmin=0,\n",
        "    vmax=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    dask_store[\"thrd_post\"].map_blocks(\n",
        "        spim.binary_closing,\n",
        "        dtype=bool\n",
        "    ).astype(np.uint8),\n",
        "    vmin=0,\n",
        "    vmax=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "da_imgs[0].mean().compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spim.iterate_structure(spim.generate_binary_structure(dask_store[\"thrd_post\"].ndim - 1, 1), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    spim.binary_opening(\n",
        "        dask_store[\"thrd_post\"][0].compute(),\n",
        "        structure=spim.iterate_structure(spim.generate_binary_structure(dask_store[\"thrd_post\"].ndim - 1, 1), 7)\n",
        "    ).astype(np.uint8),\n",
        "    vmin=0,\n",
        "    vmax=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "plt.plot(da_imgs_min)\n",
        "plt.plot(da_imgs_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROI and trace extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dask_io_remove(data_basename + postfix_rois + h5_ext, client)\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_rois_masks]\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_rois_masks_j]\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_rois_labels]\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_rois_labels_j]\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_rois]\n",
        "zarr_store.require_group(subgroup_rois)\n",
        "\n",
        "\n",
        "da_roi_masks = dask_store[subgroup_post_mask]\n",
        "\n",
        "da_lbls = da.arange(\n",
        "    1,\n",
        "    len(da_roi_masks) + 1,\n",
        "    chunks=da_roi_masks.chunks[0],\n",
        "    dtype=np.uint64\n",
        ")\n",
        "da_lblimg = (\n",
        "    da_lbls[(slice(None),) + (da_roi_masks.ndim - 1) * (None,)] * \n",
        "    da_roi_masks.astype(np.uint64)\n",
        ").max(axis=0)\n",
        "\n",
        "dask_store.update(dict(zip(\n",
        "    [subgroup_rois_masks, subgroup_rois_masks_j, subgroup_rois_labels, subgroup_rois_labels_j],\n",
        "    [da_roi_masks, da_roi_masks.astype(numpy.uint8), da_lblimg, da_lblimg.astype(numpy.uint8)]\n",
        ")))\n",
        "\n",
        "dask.distributed.progress(\n",
        "    dask.distributed.futures_of([\n",
        "        dask_store[e] for e in\n",
        "    [subgroup_rois_masks, subgroup_rois_masks_j, subgroup_rois_labels, subgroup_rois_labels_j]\n",
        "    ]),\n",
        "    notebook=False\n",
        ")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "with h5py.File(data_basename + postfix_rois + h5_ext, \"w\") as f2:\n",
        "    for k in [subgroup_rois_masks, subgroup_rois_masks_j, subgroup_rois_labels, subgroup_rois_labels_j]:\n",
        "        zarr.copy(dask_store._diskstore[k], f2)\n",
        "\n",
        "\n",
        "dask_io_remove(data_basename + postfix_traces + h5_ext, client)\n",
        "with suppress(KeyError):\n",
        "    del dask_store[subgroup_traces]\n",
        "\n",
        "\n",
        "# Load and prep data for computation.\n",
        "da_images = dask_store[subgroup_sub]\n",
        "da_masks = dask_store[subgroup_rois_masks]\n",
        "\n",
        "da_result = compute_traces(da_images, da_masks)\n",
        "\n",
        "# Store traces\n",
        "dask_store[subgroup_traces] = da_result\n",
        "\n",
        "dask.distributed.progress(dask_store[subgroup_traces], notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "with h5py.File(data_basename + postfix_traces + h5_ext, \"w\") as f2:\n",
        "    zarr.copy(dask_store._diskstore[subgroup_traces], f2)\n",
        "\n",
        "\n",
        "# View results\n",
        "imgs_min, imgs_max = 0, 100\n",
        "\n",
        "da_imgs = dask_store[subgroup_sub]\n",
        "\n",
        "da_imgs_min, da_imgs_max = da_imgs.min(), da_imgs.max()\n",
        "\n",
        "status = client.compute([da_imgs_min, da_imgs_max])\n",
        "dask.distributed.progress(status, notebook=False)\n",
        "print(\"\")\n",
        "\n",
        "imgs_min, imgs_max = [s.result() for s in status]\n",
        "\n",
        "mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "mplsv.set_images(\n",
        "    da_imgs,\n",
        "    vmin=imgs_min,\n",
        "    vmax=imgs_max\n",
        ")\n",
        "\n",
        "lblimg = dask_store[subgroup_rois_labels].compute()\n",
        "lblimg_msk = numpy.ma.masked_array(lblimg, mask=(lblimg==0))\n",
        "\n",
        "mplsv.viewer.matshow(lblimg_msk, alpha=0.3, cmap=mpl.cm.jet)\n",
        "\n",
        "\n",
        "mskimg = None\n",
        "mskimg_j = None\n",
        "lblimg = None\n",
        "traces = None\n",
        "traces_j = None\n",
        "\n",
        "del mskimg\n",
        "del mskimg_j\n",
        "del lblimg\n",
        "del traces\n",
        "del traces_j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End of workflow. Shutdown cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import distributed\n",
        "from nanshe_workflow.par import shutdown_distributed\n",
        "\n",
        "try:\n",
        "    del dask_store\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "client = distributed.client.default_client()\n",
        "\n",
        "shutdown_distributed(client)\n",
        "\n",
        "client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare interactive projection graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import textwrap\n",
        "import zlib\n",
        "\n",
        "import numpy\n",
        "import numpy as np\n",
        "\n",
        "import bokeh.plotting\n",
        "import bokeh.plotting as bp\n",
        "\n",
        "import bokeh.io\n",
        "import bokeh.io as bio\n",
        "\n",
        "import bokeh.embed\n",
        "import bokeh.embed as be\n",
        "\n",
        "from bokeh.models.mappers import LinearColorMapper\n",
        "\n",
        "import webcolors\n",
        "\n",
        "from bokeh.models import CustomJS, ColumnDataSource, HoverTool\n",
        "from bokeh.models.layouts import Row\n",
        "\n",
        "from builtins import (\n",
        "    map as imap,\n",
        "    range as irange\n",
        ")\n",
        "\n",
        "from past.builtins import basestring\n",
        "\n",
        "import nanshe_workflow\n",
        "from nanshe_workflow.data import io_remove, open_zarr\n",
        "from nanshe_workflow.vis import get_rgb_array, get_rgba_array, get_all_greys, masks_to_contours_2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mskimg = zarr_store[subgroup_rois_masks][...]\n",
        "\n",
        "traces = zarr_store[subgroup_traces][...]\n",
        "\n",
        "imgproj_mean = zarr_store[subgroup_proj_max][...]\n",
        "imgproj_max = zarr_store[subgroup_proj_mean][...]\n",
        "imgproj_std = zarr_store[subgroup_proj_std][...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result visualization\n",
        "\n",
        "* `proj_img` (`str` or `list` of `str`): which projection or projections to plot (e.g. \"max\", \"mean\", \"std\").\n",
        "* `block_size` (`int`): size of each point on any dimension in the image in terms of pixels.\n",
        "* `roi_alpha` (`float`): transparency of the ROIs in a range of [0.0, 1.0].\n",
        "* `roi_border_width` (`int`): width of the line border on each ROI.\n",
        "\n",
        "<br>\n",
        "* `trace_plot_width` (`int`): width of the trace plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proj_img = \"std\"\n",
        "block_size = 1\n",
        "roi_alpha = 0.3\n",
        "roi_border_width = 3\n",
        "trace_plot_width = 500\n",
        "\n",
        "\n",
        "bio.curdoc().clear()\n",
        "\n",
        "grey_range = get_all_greys()\n",
        "grey_cm = LinearColorMapper(grey_range)\n",
        "\n",
        "colors_rgb = get_rgb_array(len(mskimg))\n",
        "colors_rgb = colors_rgb.tolist()\n",
        "colors_rgb = list(imap(webcolors.rgb_to_hex, colors_rgb))\n",
        "\n",
        "mskctr_pts_y, mskctr_pts_x = masks_to_contours_2d(mskimg)\n",
        "\n",
        "mskctr_pts_dtype = np.min_scalar_type(max(mskimg.shape[1:]) - 1)\n",
        "mskctr_pts_y = [np.array(_, dtype=mskctr_pts_dtype) for _ in mskctr_pts_y]\n",
        "mskctr_pts_x = [np.array(_, dtype=mskctr_pts_dtype) for _ in mskctr_pts_x]\n",
        "\n",
        "mskctr_srcs = ColumnDataSource(data=dict(x=mskctr_pts_x, y=mskctr_pts_y, color=colors_rgb))\n",
        "\n",
        "\n",
        "if isinstance(proj_img, basestring):\n",
        "    proj_img = [proj_img]\n",
        "else:\n",
        "    proj_img = list(proj_img)\n",
        "\n",
        "\n",
        "proj_plot_width = block_size*mskimg.shape[2]\n",
        "proj_plot_height = block_size*mskimg.shape[1]\n",
        "plot_projs = []\n",
        "\n",
        "if \"max\" in proj_img:\n",
        "    plot_max = bp.Figure(plot_width=proj_plot_width, plot_height=proj_plot_height,\n",
        "                         x_range=[0, mskimg.shape[2]], y_range=[mskimg.shape[1], 0],\n",
        "                         tools=[\"tap\", \"pan\", \"box_zoom\", \"wheel_zoom\", \"save\", \"reset\"],\n",
        "                         title=\"Max Projection with ROIs\", border_fill_color=\"black\")\n",
        "    plot_max.image(image=[numpy.flipud(imgproj_max)], x=[0], y=[mskimg.shape[1]],\n",
        "                   dw=[imgproj_max.shape[1]], dh=[imgproj_max.shape[0]], color_mapper=grey_cm)\n",
        "    plot_max.patches('x', 'y', source=mskctr_srcs, alpha=roi_alpha, line_width=roi_border_width, color=\"color\")\n",
        "\n",
        "    plot_max.outline_line_color = \"white\"\n",
        "    for i in irange(len(plot_max.axis)):\n",
        "        plot_max.axis[i].axis_line_color = \"white\"\n",
        "\n",
        "    plot_projs.append(plot_max)\n",
        "\n",
        "\n",
        "if \"mean\" in proj_img:\n",
        "    plot_mean = bp.Figure(plot_width=proj_plot_width, plot_height=proj_plot_height,\n",
        "                         x_range=[0, mskimg.shape[2]], y_range=[mskimg.shape[1], 0],\n",
        "                         tools=[\"tap\", \"pan\", \"box_zoom\", \"wheel_zoom\", \"save\", \"reset\"],\n",
        "                         title=\"Mean Projection with ROIs\", border_fill_color=\"black\")\n",
        "    plot_mean.image(image=[numpy.flipud(imgproj_mean)], x=[0], y=[mskimg.shape[1]],\n",
        "                   dw=[mskimg.shape[2]], dh=[mskimg.shape[1]], color_mapper=grey_cm)\n",
        "    plot_mean.patches('x', 'y', source=mskctr_srcs, alpha=roi_alpha, line_width=roi_border_width, color=\"color\")\n",
        "\n",
        "    plot_mean.outline_line_color = \"white\"\n",
        "    for i in irange(len(plot_mean.axis)):\n",
        "        plot_mean.axis[i].axis_line_color = \"white\"\n",
        "\n",
        "    plot_projs.append(plot_mean)\n",
        "\n",
        "\n",
        "if \"std\" in proj_img:\n",
        "    plot_std = bp.Figure(plot_width=proj_plot_width, plot_height=proj_plot_height,\n",
        "                         x_range=[0, mskimg.shape[2]], y_range=[mskimg.shape[1], 0],\n",
        "                         tools=[\"tap\", \"pan\", \"box_zoom\", \"wheel_zoom\", \"save\", \"reset\"],\n",
        "                         title=\"Std Dev Projection with ROIs\", border_fill_color=\"black\")\n",
        "    plot_std.image(image=[numpy.flipud(imgproj_std)], x=[0], y=[mskimg.shape[1]],\n",
        "                   dw=[mskimg.shape[2]], dh=[mskimg.shape[1]], color_mapper=grey_cm)\n",
        "    plot_std.patches('x', 'y', source=mskctr_srcs, alpha=roi_alpha, line_width=roi_border_width, color=\"color\")\n",
        "\n",
        "    plot_std.outline_line_color = \"white\"\n",
        "    for i in irange(len(plot_std.axis)):\n",
        "        plot_std.axis[i].axis_line_color = \"white\"\n",
        "\n",
        "    plot_projs.append(plot_std)\n",
        "\n",
        "\n",
        "all_tr_dtype_srcs = ColumnDataSource(data=dict(traces_dtype=traces.dtype.type(0)[None]))\n",
        "all_tr_shape_srcs = ColumnDataSource(data=dict(traces_shape=traces.shape))\n",
        "all_tr_srcs = ColumnDataSource(data=dict(\n",
        "    traces=numpy.frombuffer(\n",
        "        zlib.compress(traces.tobytes()),\n",
        "        dtype=np.uint8\n",
        "    )\n",
        "))\n",
        "tr_srcs = ColumnDataSource(data=dict(times_sel=[], traces_sel=[], colors_sel=[]))\n",
        "plot_tr = bp.Figure(plot_width=trace_plot_width, plot_height=proj_plot_height,\n",
        "                    x_range=(0.0, float(traces.shape[1])), y_range=(float(traces.min()), float(traces.max())),\n",
        "                    tools=[\"pan\", \"box_zoom\", \"wheel_zoom\", \"save\", \"reset\"], title=\"ROI traces\",\n",
        "                    background_fill_color=\"black\", border_fill_color=\"black\")\n",
        "plot_tr.multi_line(\"times_sel\", \"traces_sel\", source=tr_srcs, color=\"colors_sel\")\n",
        "\n",
        "plot_tr.outline_line_color = \"white\"\n",
        "for i in irange(len(plot_tr.axis)):\n",
        "    plot_tr.axis[i].axis_line_color = \"white\"\n",
        "\n",
        "plot_projs.append(plot_tr)\n",
        "\n",
        "\n",
        "mskctr_srcs.callback = CustomJS(\n",
        "    args=dict(\n",
        "        all_tr_dtype_srcs=all_tr_dtype_srcs,\n",
        "        all_tr_shape_srcs=all_tr_shape_srcs,\n",
        "        all_tr_srcs=all_tr_srcs,\n",
        "        tr_srcs=tr_srcs\n",
        "    ), code=\"\"\"\n",
        "    var range = function(n){ return Array.from(Array(n).keys()); };\n",
        "\n",
        "    var traces_not_decoded = (all_tr_dtype_srcs.data['traces_dtype'] == 0);\n",
        "    var traces_dtype = all_tr_dtype_srcs.data['traces_dtype'].constructor;\n",
        "    var traces_shape = all_tr_shape_srcs.data['traces_shape'];\n",
        "    var trace_len = traces_shape[1];\n",
        "    var traces = all_tr_srcs.data['traces'];\n",
        "    if (traces_not_decoded) {\n",
        "        traces = window.pako.inflate(traces);\n",
        "        traces = new traces_dtype(traces.buffer);\n",
        "        all_tr_srcs.data['traces'] = traces;\n",
        "        all_tr_dtype_srcs.data['traces_dtype'] = 1;\n",
        "    }\n",
        "\n",
        "    var inds = cb_obj.selected['1d'].indices;\n",
        "    var colors = cb_obj.data['color'];\n",
        "    var selected = tr_srcs.data;\n",
        "\n",
        "    var times = range(trace_len);\n",
        "\n",
        "    selected['times_sel'] = [];\n",
        "    selected['traces_sel'] = [];\n",
        "    selected['colors_sel'] = [];\n",
        "\n",
        "    for (i = 0; i < inds.length; i++) {\n",
        "        var inds_i = inds[i];\n",
        "        var trace_i = traces.slice(trace_len*inds_i, trace_len*(inds_i+1));\n",
        "        var color_i = colors[inds_i];\n",
        "\n",
        "        selected['times_sel'].push(times);\n",
        "        selected['traces_sel'].push(trace_i);\n",
        "        selected['colors_sel'].push(color_i);\n",
        "    }\n",
        "\n",
        "    tr_srcs.change.emit();\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "plot_group = Row(*plot_projs)\n",
        "\n",
        "\n",
        "# Clear out the old HTML file before writing a new one.\n",
        "io_remove(data_basename + postfix_html + html_ext)\n",
        "\n",
        "\n",
        "def indent(text, spaces):\n",
        "    spaces = \" \" * int(spaces)\n",
        "    return \"\\n\".join(imap(lambda l: spaces + l, text.splitlines()))\n",
        "\n",
        "def write_html(filename, title, div, script, cdn):\n",
        "    html_tmplt = textwrap.dedent(u\"\"\"\\\n",
        "        <html lang=\"en\">\n",
        "            <head>\n",
        "                <meta charset=\"utf-8\">\n",
        "                <title>{title}</title>\n",
        "                {cdn}\n",
        "                <style>\n",
        "                  html {{\n",
        "                    width: 100%;\n",
        "                    height: 100%;\n",
        "                  }}\n",
        "                  body {{\n",
        "                    width: 90%;\n",
        "                    height: 100%;\n",
        "                    margin: auto;\n",
        "                    background-color: black;\n",
        "                  }}\n",
        "                </style>\n",
        "            </head>\n",
        "            <body>\n",
        "                {div}\n",
        "                {script}\n",
        "            </body>\n",
        "        </html>\n",
        "    \"\"\")\n",
        "\n",
        "    html_cont = html_tmplt.format(\n",
        "        title=title,\n",
        "        div=indent(div, 8),\n",
        "        script=indent(script, 8),\n",
        "        cdn=indent(cdn, 8),\n",
        "    )\n",
        "\n",
        "    with io.open(filename, \"w\") as fh:\n",
        "        fh.write(html_cont)\n",
        "\n",
        "script, div = be.components(plot_group)\n",
        "cdn = bokeh.resources.CDN.render() + \"\\n\"\n",
        "cdn += \"\"\"\n",
        "<script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/pako/1.0.4/pako_inflate.min.js\"></script>\n",
        "\"\"\"\n",
        "cdn += \"\\n\"\n",
        "\n",
        "write_html(data_basename + postfix_html + html_ext, data_basename + postfix_html, div, script, cdn)\n",
        "\n",
        "\n",
        "from IPython.display import display, IFrame\n",
        "display(IFrame(data_basename + postfix_html + html_ext, \"100%\", 1.05*proj_plot_height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test teardown. Ignore warnings during production runs.\n",
        "\n",
        "%run ./teardown_tests.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "widgets": {
      "state": {},
      "version": "1.2.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
