{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test setup. Ignore warnings during production runs.\n",
        "\n",
        "%run ./setup_tests.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Specify input data\n",
        "\n",
        "* `data_dir` (`str`): Where the data is located. (change if data is not in the current directory, normally is)\n",
        "* `data` (`str`): HDF5 file to use as input data.\n",
        "* `data_basename` (`str`): Basename to use for intermediate and final result files.\n",
        "* `dataset` (`str`): HDF5 dataset to use as input data.\n",
        "\n",
        "</br>\n",
        "* `num_workers` (`int`): Number of workers for iPython Cluster. (default all cores excepting one for client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"\"\n",
        "data = \"data.tif\"\n",
        "data_basename = \"data\"\n",
        "dataset = \"images\"\n",
        "\n",
        "num_workers = None\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "data_ext = os.path.splitext(data)[1].lower()\n",
        "data_dir = os.path.abspath(data_dir)\n",
        "\n",
        "postfix_trim = \"_trim\"\n",
        "postfix_dn = \"_dn\"\n",
        "postfix_reg = \"_reg\"\n",
        "postfix_sub = \"_sub\"\n",
        "postfix_f_f0 = \"_f_f0\"\n",
        "postfix_wt = \"_wt\"\n",
        "postfix_norm = \"_norm\"\n",
        "postfix_dict = \"_dict\"\n",
        "postfix_cc = \"_cc\"\n",
        "postfix_post = \"_post\"\n",
        "postfix_thrd = \"_thrd\"\n",
        "postfix_rois = \"_rois\"\n",
        "postfix_traces = \"_traces\"\n",
        "postfix_proj = \"_proj\"\n",
        "postfix_html = \"_proj\"\n",
        "\n",
        "h5_ext = os.path.extsep + \"h5\"\n",
        "tiff_ext = os.path.extsep + \"tif\"\n",
        "zarr_ext = os.path.extsep + \"zarr\"\n",
        "html_ext = os.path.extsep + \"html\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure and startup Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanshe_workflow.par import cleanup_cluster_files, get_client, set_num_workers\n",
        "\n",
        "ipypar_prof = \"sge\"\n",
        "\n",
        "num_workers = set_num_workers(num_workers)\n",
        "\n",
        "cleanup_cluster_files(ipypar_prof)\n",
        "\n",
        "from sys import executable as PYTHON\n",
        "!$PYTHON -m ipyparallel.apps.ipclusterapp start --daemon --profile=$ipypar_prof\n",
        "del PYTHON\n",
        "\n",
        "client = get_client(ipypar_prof)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from builtins import range as irange\n",
        "\n",
        "def getcwdi(i):\n",
        "    from os import getcwd\n",
        "    return getcwd()\n",
        "\n",
        "while not all(map(lambda p: p == data_dir, [os.getcwd()] + client[:].map(getcwdi, irange(len(client))).get())):\n",
        "    os.chdir(data_dir)\n",
        "    client[:].map(os.chdir, len(client) * [data_dir]).get()\n",
        "\n",
        "del getcwdi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define functions for computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.cm\n",
        "import matplotlib.pyplot\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mplview.core import MatplotlibViewer as MPLViewer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client[:].use_cloudpickle().get()\n",
        "\n",
        "with client[:].sync_imports():\n",
        "    import collections\n",
        "    import contextlib\n",
        "    import copy\n",
        "    import functools\n",
        "    import gc\n",
        "    import inspect\n",
        "    import itertools\n",
        "    import logging\n",
        "    import math\n",
        "    import numbers\n",
        "    import os\n",
        "    import sys\n",
        "\n",
        "    from contextlib import contextmanager\n",
        "\n",
        "    from builtins import range as irange\n",
        "\n",
        "    import numpy\n",
        "    import scipy\n",
        "    import scipy.ndimage\n",
        "    import h5py\n",
        "\n",
        "    import numpy as np\n",
        "    import scipy as sp\n",
        "    import scipy.ndimage as spim\n",
        "    import h5py as hp\n",
        "\n",
        "    import dask\n",
        "    import dask.array\n",
        "    import dask.array.fft\n",
        "    import dask.utils\n",
        "    import dask.distributed\n",
        "\n",
        "    import dask.array as da\n",
        "\n",
        "    import dask_imread\n",
        "    import dask_ndfilters\n",
        "    import dask_ndfourier\n",
        "    import dask_ndmeasure\n",
        "\n",
        "    from toolz import sliding_window\n",
        "\n",
        "    import zarr\n",
        "\n",
        "    import imgroi\n",
        "    import imgroi.core\n",
        "    from imgroi.core import label_mask_stack\n",
        "\n",
        "    import nanshe\n",
        "    from nanshe.imp.segment import generate_dictionary\n",
        "\n",
        "    import nanshe_workflow\n",
        "    from nanshe_workflow.data import io_remove, dask_load_hdf5, zip_zarr, open_zarr, DataBlocks, LazyZarrDataset\n",
        "    from nanshe_workflow.par import get_executor\n",
        "\n",
        "zarr.blosc.set_nthreads(1)\n",
        "zarr.blosc.use_threads = False\n",
        "client[:].apply(zarr.blosc.set_nthreads, 1).get();\n",
        "client[:].apply(setattr, zarr.blosc, \"use_threads\", False).get();\n",
        "\n",
        "logging.getLogger(\"nanshe\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanshe_workflow.data import hdf5_to_zarr, zarr_to_hdf5\n",
        "from nanshe_workflow.data import save_tiff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import pyfftw.interfaces.numpy_fft as numpy_fft\n",
        "except ImportError:\n",
        "    import numpy.fft as numpy_fft\n",
        "\n",
        "rfftn = da.fft.fft_wrap(numpy_fft.rfftn)\n",
        "irfftn = da.fft.fft_wrap(numpy_fft.irfftn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanshe_workflow.par import halo_block_parallel\n",
        "\n",
        "from nanshe_workflow.imp2 import extract_f0, wavelet_transform, normalize_data\n",
        "\n",
        "from nanshe_workflow.par import halo_block_generate_dictionary_parallel\n",
        "from nanshe_workflow.imp import block_postprocess_data_parallel\n",
        "\n",
        "par_generate_dictionary = halo_block_generate_dictionary_parallel(client, None)(generate_dictionary)\n",
        "par_postprocess_data = block_postprocess_data_parallel(client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanshe_workflow.par import frame_stack_calculate_parallel\n",
        "\n",
        "from nanshe_workflow.proj2 import compute_traces\n",
        "\n",
        "from nanshe_workflow.proj2 import compute_adj_harmonic_mean_projection\n",
        "from nanshe_workflow.proj2 import compute_min_projection\n",
        "from nanshe_workflow.proj2 import compute_max_projection\n",
        "\n",
        "from nanshe_workflow.proj2 import compute_moment_projections\n",
        "\n",
        "from nanshe_workflow.proj2 import norm_layer\n",
        "\n",
        "from nanshe_workflow.proj import stack_norm_layer_parallel\n",
        "from nanshe_workflow.proj import stack_compute_min_projection_parallel\n",
        "from nanshe_workflow.proj import stack_compute_max_projection_parallel\n",
        "\n",
        "par_norm_layer = frame_stack_calculate_parallel(client, stack_norm_layer_parallel)\n",
        "par_compute_min_projection = frame_stack_calculate_parallel(client, stack_compute_min_projection_parallel)\n",
        "par_compute_max_projection = frame_stack_calculate_parallel(client, stack_compute_max_projection_parallel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Begin workflow. Set parameters and run each cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert TIFF/HDF5 to Zarr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "io_remove(data_basename + zarr_ext)\n",
        "with open_zarr(data_basename + zarr_ext, \"w\") as f1:\n",
        "    with get_executor(client) as executor:\n",
        "        if data_ext == tiff_ext:\n",
        "            a = dask_imread.imread(data)\n",
        "        elif data_ext == h5_ext:\n",
        "            a = dask_load_hdf5(data, dataset)\n",
        "\n",
        "        d = f1.create_dataset(\n",
        "            dataset,\n",
        "            shape=a.shape,\n",
        "            dtype=a.dtype,\n",
        "            chunks=True\n",
        "        )\n",
        "        a = a.rechunk(d.chunks)\n",
        "        status = executor.compute(da.store(a, d, lock=False, compute=False))\n",
        "        dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "        del a\n",
        "        del d\n",
        "\n",
        "zip_zarr(data_basename + zarr_ext)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Input Data\n",
        "\n",
        "* `norm_frames` (`int`): number of frames for use during normalization of each full frame block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "norm_frames = 100\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + zarr_ext, dataset)\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=par_compute_min_projection(num_frames=norm_frames)(result_image_stack).min(),\n",
        "        vmax=par_compute_max_projection(num_frames=norm_frames)(result_image_stack).max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trimming\n",
        "\n",
        "* `front` (`int`): amount to trim off the front\n",
        "* `back` (`int`): amount to trim off the back\n",
        "\n",
        "<br>\n",
        "* `block_frames` (`int`): number of frames to work with in each block (run in parallel).\n",
        "* `norm_frames` (`int`): number of frames for use during normalization of each full frame block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "front = 0\n",
        "back = 0\n",
        "\n",
        "block_frames = 1\n",
        "norm_frames = 100\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_trim + zarr_ext)\n",
        "io_remove(data_basename + postfix_trim + h5_ext)\n",
        "\n",
        "\n",
        "with open_zarr(data_basename + zarr_ext, \"r\") as f:\n",
        "    with get_executor(client) as executor:\n",
        "        # Load and prep data for computation.\n",
        "        imgs = f[\"images\"]\n",
        "        da_imgs = da.from_array(imgs, chunks=(block_frames,) + imgs.shape[1:])\n",
        "\n",
        "        # Trim frames from front and back\n",
        "        da_imgs_trim = da_imgs[front:len(da_imgs)-back]\n",
        "\n",
        "        # Store denoised data\n",
        "        with open_zarr(data_basename + postfix_trim + zarr_ext, \"w\") as f2:\n",
        "            result = f2.create_dataset(\n",
        "                \"images\",\n",
        "                shape=da_imgs_trim.shape,\n",
        "                dtype=da_imgs_trim.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_imgs_trim = da_imgs_trim.rechunk(result.chunks)\n",
        "            status = executor.compute(da.store(da_imgs_trim, result, lock=False, compute=False))\n",
        "            dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_trim + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_trim + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_trim + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_trim + zarr_ext, dataset)\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=par_compute_min_projection(num_frames=norm_frames)(result_image_stack).min(),\n",
        "        vmax=par_compute_max_projection(num_frames=norm_frames)(result_image_stack).max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Denoising\n",
        "\n",
        "* `med_filt_size` (`int`): footprint size for median filter\n",
        "* `norm_filt_sigma` (`int`/`float`): sigma for Gaussian filter\n",
        "\n",
        "<br>\n",
        "* `block_frames` (`int`): number of frames to work with in each block (run in parallel).\n",
        "* `norm_frames` (`int`): number of frames for use during normalization of each full frame block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "med_filt_size = 3\n",
        "norm_filt_sigma = 10\n",
        "\n",
        "block_frames = 1\n",
        "norm_frames = 100\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_dn + zarr_ext)\n",
        "io_remove(data_basename + postfix_dn + h5_ext)\n",
        "\n",
        "\n",
        "with open_zarr(data_basename + postfix_trim + zarr_ext, \"r\") as f:\n",
        "    with get_executor(client) as executor:\n",
        "        # Load and prep data for computation.\n",
        "        imgs = f[\"images\"]\n",
        "        da_imgs = da.from_array(imgs, chunks=(block_frames,) + imgs.shape[1:])\n",
        "\n",
        "        da_imgs_flt = da_imgs\n",
        "        if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "                da_imgs_flt.dtype.itemsize >= 4):\n",
        "            da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "        # Median filter frames\n",
        "        da_imgs_medf = dask_ndfilters.median_filter(\n",
        "            da_imgs_flt, (1,) + (da_imgs_flt.ndim - 1) * (med_filt_size,)\n",
        "        )\n",
        "\n",
        "        # Compute the Gaussian filter of frames\n",
        "        da_imgs_smoothed = dask_ndfilters.gaussian_filter(\n",
        "            da_imgs_medf, (0,) + (da_imgs_medf.ndim - 1) * (norm_filt_sigma,)\n",
        "        )\n",
        "\n",
        "        # Apply high pass filter to images\n",
        "        da_imgs_filt = da_imgs_medf - da_imgs_smoothed\n",
        "\n",
        "        # Reset minimum to original value.\n",
        "        da_imgs_filt += da_imgs.min() - da_imgs_filt.min()\n",
        "\n",
        "        # Store denoised data\n",
        "        with open_zarr(data_basename + postfix_dn + zarr_ext, \"w\") as f2:\n",
        "            result = f2.create_dataset(\n",
        "                \"images\",\n",
        "                shape=da_imgs_filt.shape,\n",
        "                dtype=da_imgs_filt.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_imgs_filt = da_imgs_filt.rechunk(result.chunks)\n",
        "            status = executor.compute(da.store(da_imgs_filt, result, lock=False, compute=False))\n",
        "            dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_dn + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_dn + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_dn + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_dn + zarr_ext, dataset)\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=par_compute_min_projection(num_frames=norm_frames)(result_image_stack).min(),\n",
        "        vmax=par_compute_max_projection(num_frames=norm_frames)(result_image_stack).max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Registration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_best_match(matches):\n",
        "    i = numpy.argmin((matches ** 2).sum(axis=0))\n",
        "\n",
        "    return matches[:, i]\n",
        "\n",
        "\n",
        "def compute_offset(match_mask):\n",
        "    frame_shape = np.array(match_mask.shape)\n",
        "    half_frame_shape = frame_shape // 2\n",
        "\n",
        "    matches = np.array(match_mask.nonzero())\n",
        "    if matches.size == 0:\n",
        "        matches = np.array([[0], [0]])\n",
        "\n",
        "    above = (matches > half_frame_shape[:, None]).astype(matches.dtype)\n",
        "    matches -= above * frame_shape[:, None]\n",
        "\n",
        "    return find_best_match(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "num_reps = 0\n",
        "tmpl_hist_wght = 0.25\n",
        "thld_rel_dist = 0.0\n",
        "\n",
        "block_frames = 1\n",
        "norm_frames = 100\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_reg + zarr_ext)\n",
        "io_remove(data_basename + postfix_reg + h5_ext)\n",
        "\n",
        "\n",
        "with open_zarr(data_basename + postfix_dn + zarr_ext, \"r\") as f:\n",
        "    with get_executor(client) as executor:\n",
        "        # Load and prep data for computation.\n",
        "        imgs = f[\"images\"]\n",
        "        da_imgs = da.from_array(imgs, chunks=(block_frames,) + imgs.shape[1:])\n",
        "\n",
        "        da_imgs_flt = da_imgs\n",
        "        if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "                da_imgs_flt.dtype.itemsize >= 4):\n",
        "            da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "        # Create frame array\n",
        "        frame_shape = da.from_array(\n",
        "            np.array(da_imgs_flt.shape[1:], dtype=int),\n",
        "            chunks=(da_imgs_flt.ndim - 1,)\n",
        "        )\n",
        "\n",
        "        # Persist frame shape\n",
        "        frame_shape = frame_shape.persist()\n",
        "\n",
        "        # Compute the FFT of frames\n",
        "        da_imgs_fft = rfftn(da_imgs_flt, axes=tuple(irange(1, imgs.ndim)))\n",
        "\n",
        "        # Persist FFT of frames\n",
        "        da_imgs_fft = da_imgs_fft.persist()\n",
        "\n",
        "        # Initialize\n",
        "        i = 0\n",
        "        avg_rel_dist = 1.0\n",
        "        tmpl_hist_wght = da_imgs_flt.dtype.type(tmpl_hist_wght)\n",
        "        shifts = da.zeros(\n",
        "            (len(da_imgs_flt), da_imgs_flt.ndim - 1),\n",
        "            dtype=int,\n",
        "            chunks=(1, da_imgs_flt.ndim - 1)\n",
        "        )\n",
        "        da_imgs_fft_tmplt = da_imgs_fft.mean(axis=0)\n",
        "\n",
        "        # Persist FFT template image\n",
        "        da_imgs_fft_tmplt = da_imgs_fft_tmplt.persist()\n",
        "\n",
        "        while avg_rel_dist > thld_rel_dist and i < num_reps:\n",
        "            # Compute the shifted frames\n",
        "            shifted_frames = []\n",
        "            for j in irange(len(da_imgs_fft)):\n",
        "                shifted_frames.append(dask_ndfourier.fourier_shift(\n",
        "                    da_imgs_fft[i], shifts[i]\n",
        "                ))\n",
        "            shifted_frames = da.stack(shifted_frames)\n",
        "\n",
        "            # Compute the template FFT\n",
        "            da_imgs_fft_tmplt = (\n",
        "                tmpl_hist_wght * da_imgs_fft_tmplt +\n",
        "                (1 - tmpl_hist_wght) * shifted_frames.mean(axis=0)\n",
        "            )\n",
        "\n",
        "            # Free connected persisted values\n",
        "            del shifted_frames\n",
        "\n",
        "            # Persist FFT template image\n",
        "            da_imgs_fft_tmplt = da_imgs_fft_tmplt.persist()\n",
        "\n",
        "            # Find the best overlap with the template.\n",
        "            overlap = irfftn(\n",
        "                da_imgs_fft * da_imgs_fft_tmplt[None],\n",
        "                s=da_imgs_flt.shape[1:],\n",
        "                axes=tuple(irange(1, imgs.ndim))\n",
        "            )\n",
        "            overlap_max = overlap.max(axis=tuple(irange(1, imgs.ndim)))\n",
        "            overlap_max_match = (overlap == overlap_max[(Ellipsis,) + (None,) * (imgs.ndim - 1)])\n",
        "\n",
        "            # Compute the shift for each frame.\n",
        "            old_shifts = shifts\n",
        "            shifts = []\n",
        "            for j in irange(len(overlap_max_match)):\n",
        "                shift_j = dask.delayed(compute_offset)(overlap_max_match[j])\n",
        "                shift_j = da.from_delayed(shift_j, (2,), int)\n",
        "                shifts.append(shift_j)\n",
        "                del shift_j\n",
        "            shifts = da.stack(shifts)\n",
        "\n",
        "            # Free connected persisted values\n",
        "            del overlap\n",
        "            del overlap_max\n",
        "            del overlap_max_match\n",
        "\n",
        "            # Remove any collective frame drift.\n",
        "            drift = shifts.mean(axis=0).round().astype(shifts.dtype)\n",
        "            shifts = shifts - drift[None]\n",
        "\n",
        "            # Free connected persisted values\n",
        "            del drift\n",
        "\n",
        "            # Persist shifts\n",
        "            shifts = shifts.persist()\n",
        "\n",
        "            # Find shift change.\n",
        "            diff_shifts = shifts - old_shifts\n",
        "            rel_diff_shifts = (\n",
        "                diff_shifts.astype(da_imgs_flt.dtype) / \n",
        "                frame_shape.astype(da_imgs_flt.dtype) /\n",
        "                (da_imgs_flt.dtype.type(len(frame_shape)) ** 0.5)\n",
        "            )\n",
        "            rel_dist_shifts = (rel_diff_shifts ** 2.0).sum(axis=1) ** 0.5\n",
        "            avg_rel_dist = rel_dist_shifts.sum() / da_imgs_flt.dtype.type(len(shifts))\n",
        "\n",
        "            # Free old shifts\n",
        "            del old_shifts\n",
        "\n",
        "            # Free connected persisted values\n",
        "            del diff_shifts\n",
        "            del rel_diff_shifts\n",
        "            del rel_dist_shifts\n",
        "\n",
        "            # Compute change\n",
        "            status = executor.compute(avg_rel_dist)\n",
        "            dask.distributed.progress(status, notebook=False)\n",
        "            avg_rel_dist = status.result()\n",
        "            i += 1\n",
        "\n",
        "            # Show change\n",
        "            print(\"\")\n",
        "            print((i, avg_rel_dist))\n",
        "\n",
        "        # Drop unneeded items\n",
        "        del frame_shape\n",
        "        del da_imgs_flt\n",
        "        del da_imgs_fft\n",
        "        del da_imgs_fft_tmplt\n",
        "\n",
        "        # Truncate shifted part of each frame\n",
        "        da_imgs_trunc = []\n",
        "        da_imgs_trunc_shape = da_imgs.shape[1:]\n",
        "        for i in irange(len(da_imgs)):\n",
        "            slice_i = [i]\n",
        "            for j in irange(shifts.shape[1]):\n",
        "                shifts_ij = numpy.array(shifts[i, j])[()]\n",
        "                if shifts_ij < 0:\n",
        "                    slice_i.append(slice(-shifts_ij, None))\n",
        "                elif shifts_ij > 0:\n",
        "                    slice_i.append(slice(None, -shifts_ij))\n",
        "                else:\n",
        "                    slice_i.append(slice(None))\n",
        "            slice_i = tuple(slice_i)\n",
        "            da_imgs_trunc.append(da_imgs[slice_i])\n",
        "            da_imgs_trunc_shape = tuple(np.minimum(\n",
        "                da_imgs_trunc_shape, da_imgs_trunc[-1].shape\n",
        "            ))\n",
        "\n",
        "        # Free raw data and shifts\n",
        "        del da_imgs\n",
        "        del shifts\n",
        "\n",
        "        # Truncate all frames to smallest one\n",
        "        da_imgs_trunc_cut = tuple(map(\n",
        "            lambda s: slice(None, s), da_imgs_trunc_shape\n",
        "        ))\n",
        "        for i in irange(len(da_imgs_trunc)):\n",
        "            da_imgs_trunc[i] = da_imgs_trunc[i][da_imgs_trunc_cut]\n",
        "        da_imgs_trunc = da.stack(da_imgs_trunc)\n",
        "\n",
        "        # Store registered data\n",
        "        with open_zarr(data_basename + postfix_reg + zarr_ext, \"w\") as f2:\n",
        "            result = f2.create_dataset(\n",
        "                \"images\",\n",
        "                shape=da_imgs_trunc.shape,\n",
        "                dtype=da_imgs_trunc.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_imgs_trunc = da_imgs_trunc.rechunk(result.chunks)\n",
        "            status = executor.compute(da.store(da_imgs_trunc, result, lock=False, compute=False))\n",
        "            dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "        # Free truncated frames\n",
        "        del da_imgs_trunc\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_reg + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_reg + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_reg + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_reg + zarr_ext, dataset)\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=par_compute_min_projection(num_frames=norm_frames)(result_image_stack).min(),\n",
        "        vmax=par_compute_max_projection(num_frames=norm_frames)(result_image_stack).max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Projections\n",
        "\n",
        "* `block_frames` (`int`): number of frames to work with in each block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "block_frames = 100\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_proj + zarr_ext)\n",
        "io_remove(data_basename + postfix_proj + h5_ext)\n",
        "\n",
        "\n",
        "with open_zarr(data_basename + postfix_reg + zarr_ext, \"r\") as f:\n",
        "    with get_executor(client) as executor:\n",
        "        # Load and prep data for computation.\n",
        "        imgs = f[\"images\"]\n",
        "        da_imgs = da.from_array(imgs, chunks=(block_frames,) + imgs.shape[1:])\n",
        "\n",
        "        da_imgs_flt = da_imgs\n",
        "        if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "                da_imgs_flt.dtype.itemsize >= 4):\n",
        "            da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "        da_imgs_proj_hmean = compute_adj_harmonic_mean_projection(da_imgs_flt)\n",
        "\n",
        "        da_imgs_proj_max = compute_max_projection(da_imgs_flt)\n",
        "\n",
        "        da_imgs_proj_mean, da_imgs_proj_std = compute_moment_projections(da_imgs_flt, 3)[1:]\n",
        "        da_imgs_proj_std -= da_imgs_proj_mean**2\n",
        "        da_imgs_proj_std = da.sqrt(da_imgs_proj_std)\n",
        "\n",
        "        # Store denoised data\n",
        "        with open_zarr(data_basename + postfix_proj + zarr_ext, \"w\") as f2:\n",
        "            statuses = []\n",
        "\n",
        "            zarr_proj_hmean = f2.create_dataset(\n",
        "                \"hmean\",\n",
        "                shape=da_imgs_proj_hmean.shape,\n",
        "                dtype=da_imgs_proj_hmean.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_imgs_proj_hmean = da_imgs_proj_hmean.rechunk(zarr_proj_hmean.chunks)\n",
        "            statuses.append(executor.compute(\n",
        "                da.store(da_imgs_proj_hmean, zarr_proj_hmean, lock=False, compute=False\n",
        "            )))\n",
        "\n",
        "            zarr_proj_max = f2.create_dataset(\n",
        "                \"max\",\n",
        "                shape=da_imgs_proj_max.shape,\n",
        "                dtype=da_imgs_proj_max.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_imgs_proj_max = da_imgs_proj_max.rechunk(zarr_proj_max.chunks)\n",
        "            statuses.append(executor.compute(\n",
        "                da.store(da_imgs_proj_max, zarr_proj_max, lock=False, compute=False\n",
        "            )))\n",
        "\n",
        "            zarr_proj_mean = f2.create_dataset(\n",
        "                \"mean\",\n",
        "                shape=da_imgs_proj_mean.shape,\n",
        "                dtype=da_imgs_proj_mean.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_imgs_proj_mean = da_imgs_proj_mean.rechunk(zarr_proj_mean.chunks)\n",
        "            statuses.append(executor.compute(\n",
        "                da.store(da_imgs_proj_mean, zarr_proj_mean, lock=False, compute=False\n",
        "            )))\n",
        "\n",
        "            zarr_proj_std = f2.create_dataset(\n",
        "                \"std\",\n",
        "                shape=da_imgs_proj_std.shape,\n",
        "                dtype=da_imgs_proj_std.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_imgs_proj_std = da_imgs_proj_std.rechunk(zarr_proj_std.chunks)\n",
        "            statuses.append(executor.compute(\n",
        "                da.store(da_imgs_proj_std, zarr_proj_std, lock=False, compute=False\n",
        "            )))\n",
        "\n",
        "            dask.distributed.progress(statuses, notebook=False)\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_proj + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_proj + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_proj + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subtract Projection\n",
        "\n",
        "* `block_frames` (`int`): number of frames to work with in each block (run in parallel).\n",
        "* `norm_frames` (`int`): number of frames for use during normalization of each full frame block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "block_frames = 100\n",
        "norm_frames = 100\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_sub + zarr_ext)\n",
        "io_remove(data_basename + postfix_sub + h5_ext)\n",
        "\n",
        "\n",
        "with open_zarr(data_basename + postfix_reg + zarr_ext, \"r\") as f:\n",
        "    with get_executor(client) as executor:\n",
        "        # Load and prep data for computation.\n",
        "        imgs = f[\"images\"]\n",
        "        da_imgs = da.from_array(imgs, chunks=(block_frames,) + imgs.shape[1:])\n",
        "\n",
        "        da_imgs_flt = da_imgs\n",
        "        if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "                da_imgs_flt.dtype.itemsize >= 4):\n",
        "            da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "        da_imgs_adj = da_imgs_flt.min() - 1\n",
        "        da_imgs_flt_shifted = da_imgs_flt - da_imgs_adj\n",
        "        da_imgs_hmean = da_imgs_adj + 1 / (1 / da_imgs_flt_shifted).mean(axis=0)\n",
        "\n",
        "        da_imgs_sub = da_imgs_flt - da_imgs_hmean\n",
        "        da_imgs_sub -= da_imgs_sub.min()\n",
        "\n",
        "        # Store denoised data\n",
        "        with open_zarr(data_basename + postfix_sub + zarr_ext, \"w\") as f2:\n",
        "            result = f2.create_dataset(\n",
        "                \"images\",\n",
        "                shape=da_imgs_sub.shape,\n",
        "                dtype=da_imgs_sub.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_imgs_sub = da_imgs_sub.rechunk(result.chunks)\n",
        "            status = executor.compute(da.store(da_imgs_sub, result, lock=False, compute=False))\n",
        "            dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_sub + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_sub + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_sub + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_sub + zarr_ext, \"images\")\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=par_compute_min_projection(num_frames=norm_frames)(result_image_stack).min(),\n",
        "        vmax=par_compute_max_projection(num_frames=norm_frames)(result_image_stack).max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Background Subtraction\n",
        "\n",
        "* `half_window_size` (`int`): the rank filter window size is `2*half_window_size+1`.\n",
        "* `which_quantile` (`float`): which quantile to return from the rank filter.\n",
        "* `temporal_smoothing_gaussian_filter_stdev` (`float`): stdev for gaussian filter to convolve over time.\n",
        "* `temporal_smoothing_gaussian_filter_window_size` (`float`): window for gaussian filter to convolve over time. (Measured in standard deviations)\n",
        "* `spatial_smoothing_gaussian_filter_stdev` (`float`): stdev for gaussian filter to convolve over space.\n",
        "* `spatial_smoothing_gaussian_filter_window_size` (`float`): window for gaussian filter to convolve over space. (Measured in standard deviations)\n",
        "\n",
        "<br>\n",
        "* `block_frames` (`int`): number of frames to work with in each block (run in parallel).\n",
        "* `block_space` (`int`): extent of each spatial dimension for each block (run in parallel).\n",
        "* `norm_frames` (`int`): number of frames for use during normalization of each full frame block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "half_window_size = 100\n",
        "which_quantile = 0.5\n",
        "temporal_smoothing_gaussian_filter_stdev = 0.0\n",
        "temporal_smoothing_gaussian_filter_window_size = 0\n",
        "spatial_smoothing_gaussian_filter_stdev = 0.0\n",
        "spatial_smoothing_gaussian_filter_window_size = 0\n",
        "\n",
        "block_frames = 1000\n",
        "block_space = 100\n",
        "norm_frames = 100\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_f_f0 + zarr_ext)\n",
        "io_remove(data_basename + postfix_f_f0 + h5_ext)\n",
        "\n",
        "\n",
        "with open_zarr(data_basename + postfix_sub + zarr_ext, \"r\") as f:\n",
        "    with get_executor(client) as executor:\n",
        "        # Load and prep data for computation.\n",
        "        imgs = f[\"images\"]\n",
        "        da_imgs = da.from_array(\n",
        "            imgs, chunks=(block_frames,) + (imgs.ndim - 1) * (block_space,)\n",
        "        )\n",
        "\n",
        "        da_imgs_flt = da_imgs\n",
        "        if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "                da_imgs_flt.dtype.itemsize >= 4):\n",
        "            da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "        bias = 1 - da_imgs_flt.min()\n",
        "\n",
        "        da_result = extract_f0(\n",
        "            da_imgs_flt,\n",
        "            half_window_size=half_window_size,\n",
        "            which_quantile=which_quantile,\n",
        "            temporal_smoothing_gaussian_filter_stdev=temporal_smoothing_gaussian_filter_stdev,\n",
        "            temporal_smoothing_gaussian_filter_window_size=temporal_smoothing_gaussian_filter_window_size,\n",
        "            spatial_smoothing_gaussian_filter_stdev=spatial_smoothing_gaussian_filter_stdev,\n",
        "            spatial_smoothing_gaussian_filter_window_size=spatial_smoothing_gaussian_filter_window_size,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "        # Store denoised data\n",
        "        with open_zarr(data_basename + postfix_f_f0 + zarr_ext, \"w\") as f2:\n",
        "            result = f2.create_dataset(\n",
        "                \"images\",\n",
        "                shape=da_result.shape,\n",
        "                dtype=da_result.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_result = da_result.rechunk(result.chunks)\n",
        "            status = executor.compute(da.store(da_result, result, lock=False, compute=False))\n",
        "            dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_f_f0 + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_f_f0 + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_f_f0 + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_f_f0 + zarr_ext, \"images\")\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=par_compute_min_projection(num_frames=norm_frames)(result_image_stack).min(),\n",
        "        vmax=par_compute_max_projection(num_frames=norm_frames)(result_image_stack).max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wavelet Transform\n",
        "\n",
        "* `scale` (`int`): the scale of wavelet transform to apply.\n",
        "\n",
        "<br>\n",
        "* `block_frames` (`int`): number of frames to work with in each block (run in parallel).\n",
        "* `block_space` (`int`): extent of each spatial dimension for each block (run in parallel).\n",
        "* `norm_frames` (`int`): number of frames for use during normalization of each full frame block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "scale = 3\n",
        "\n",
        "block_frames = 200\n",
        "block_space = 300\n",
        "norm_frames = 100\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_wt + zarr_ext)\n",
        "io_remove(data_basename + postfix_wt + h5_ext)\n",
        "\n",
        "\n",
        "with open_zarr(data_basename + postfix_f_f0 + zarr_ext, \"r\") as f:\n",
        "    with get_executor(client) as executor:\n",
        "        # Load and prep data for computation.\n",
        "        imgs = f[\"images\"]\n",
        "        da_imgs = da.from_array(\n",
        "            imgs, chunks=(block_frames,) + (imgs.ndim - 1) * (block_space,)\n",
        "        )\n",
        "\n",
        "        da_imgs_flt = da_imgs\n",
        "        if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "                da_imgs_flt.dtype.itemsize >= 4):\n",
        "            da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "        da_result = wavelet_transform(\n",
        "            da_imgs,\n",
        "            scale=scale\n",
        "        )\n",
        "\n",
        "        # Store denoised data\n",
        "        with open_zarr(data_basename + postfix_wt + zarr_ext, \"w\") as f2:\n",
        "            result = f2.create_dataset(\n",
        "                \"images\",\n",
        "                shape=da_result.shape,\n",
        "                dtype=da_result.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_result = da_result.rechunk(result.chunks)\n",
        "            status = executor.compute(da.store(da_result, result, lock=False, compute=False))\n",
        "            dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_wt + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_wt + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_wt + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_wt + zarr_ext, \"images\")\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=par_compute_min_projection(num_frames=norm_frames)(result_image_stack).min(),\n",
        "        vmax=par_compute_max_projection(num_frames=norm_frames)(result_image_stack).max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Project\n",
        "\n",
        "* `proj_type` (`str`): type of projection to take.\n",
        "\n",
        "<br>\n",
        "\n",
        "* `block_frames` (`int`): number of frames to work with in each full frame block (run in parallel).\n",
        "* `block_space` (`int`): extent of each spatial dimension for each block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "proj_type = \"max\"\n",
        "\n",
        "block_frames = 40\n",
        "block_space = 300\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_dict + zarr_ext)\n",
        "io_remove(data_basename + postfix_dict + h5_ext)\n",
        "\n",
        "\n",
        "with open_zarr(data_basename + postfix_wt + zarr_ext, \"r\") as f:\n",
        "    with get_executor(client) as executor:\n",
        "        # Load and prep data for computation.\n",
        "        imgs = f[\"images\"]\n",
        "        da_imgs = da.from_array(\n",
        "            imgs, chunks=(block_frames,) + (imgs.ndim - 1) * (block_space,)\n",
        "        )\n",
        "\n",
        "        da_imgs_flt = da_imgs\n",
        "        if not (issubclass(da_imgs_flt.dtype.type, np.floating) and \n",
        "                da_imgs_flt.dtype.itemsize >= 4):\n",
        "            da_imgs_flt = da_imgs_flt.astype(np.float32)\n",
        "\n",
        "        da_result = da_imgs\n",
        "        if proj_type == \"max\":\n",
        "            da_result = da_result.max(axis=0, keepdims=True)\n",
        "        elif proj_type == \"std\":\n",
        "            da_result = da_result.std(axis=0, keepdims=True)\n",
        "\n",
        "        # Store denoised data\n",
        "        with open_zarr(data_basename + postfix_dict + zarr_ext, \"w\") as f2:\n",
        "            result = f2.create_dataset(\n",
        "                \"images\",\n",
        "                shape=da_result.shape,\n",
        "                dtype=da_result.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_result = da_result.rechunk(result.chunks)\n",
        "            status = executor.compute(da.store(da_result, result, lock=False, compute=False))\n",
        "            dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_dict + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_dict + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_dict + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_dict + zarr_ext, \"images\")[...][...]\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=result_image_stack.min(),\n",
        "        vmax=result_image_stack.max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Connected components\n",
        "\n",
        "* `significance_threshold` (`float`): number of standard deviations below which to include in \"noise\" estimate\n",
        "* `noise_threshold` (`float`): number of units of \"noise\" above which something needs to be to be significant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "significance_threshold = 3.0\n",
        "noise_threshold = 1.0\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_cc + zarr_ext)\n",
        "io_remove(data_basename + postfix_cc + h5_ext)\n",
        "\n",
        "\n",
        "with open_zarr(data_basename + postfix_dict + zarr_ext, \"r\") as f:\n",
        "    with get_executor(client) as executor:\n",
        "        imgs = f[\"images\"]\n",
        "        da_imgs = da.from_array(\n",
        "            imgs, chunks=imgs.shape\n",
        "        )\n",
        "        da_imgs = da_imgs[0]\n",
        "\n",
        "        da_imgs_thrd = (da_imgs - noise_threshold * (da_imgs - significance_threshold * da_imgs.std()).std()) > 0\n",
        "\n",
        "        da_lbl_img, da_num_lbls = dask_ndmeasure.label(da_imgs_thrd)\n",
        "        da_lbl_img, da_num_lbls = executor.persist([da_lbl_img, da_num_lbls])\n",
        "\n",
        "        da_result = []\n",
        "        for i in irange(1, 1 + int(da_num_lbls)):\n",
        "            da_result.append((da_lbl_img == i)[None])\n",
        "        da_result = da.concatenate(da_result)\n",
        "\n",
        "        with open_zarr(data_basename + postfix_cc + zarr_ext, \"w\") as f2:\n",
        "            result = f2.create_group(\"rois\").create_dataset(\n",
        "                \"mask\",\n",
        "                shape=da_result.shape,\n",
        "                dtype=da_result.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_result = da_result.rechunk(result.chunks)\n",
        "            status = executor.compute(da.store(da_result, result, lock=False, compute=False))\n",
        "            dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_cc + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_cc + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_cc + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_cc + zarr_ext, \"rois/mask\")[...][...].astype(np.uint8)\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=result_image_stack.min(),\n",
        "        vmax=result_image_stack.max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROI Refinement\n",
        "\n",
        "* `area_min_threshold` (`float`): minimum area required for all ROIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "area_min_threshold = 20.0\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_post + zarr_ext)\n",
        "io_remove(data_basename + postfix_post + h5_ext)\n",
        "\n",
        "\n",
        "with open_zarr(data_basename + postfix_cc + zarr_ext, \"r\") as f:\n",
        "    with get_executor(client) as executor:\n",
        "        imgs = f[\"rois/mask\"]\n",
        "        da_imgs = da.from_array(\n",
        "            imgs, chunks=imgs.shape\n",
        "        )\n",
        "\n",
        "        da_num_lbls = len(da_imgs)\n",
        "        da_lbl_img = (\n",
        "            np.arange(1, 1 + da_num_lbls)[(slice(None),) + da_lbl_img.ndim * (None,)] * da_imgs\n",
        "        ).sum(axis=0)\n",
        "\n",
        "        da_area_lbls = dask_ndmeasure.sum(\n",
        "            da.ones(da_lbl_img.shape, dtype=int, chunks=da_lbl_img.chunks),\n",
        "            da_lbl_img,\n",
        "            list(irange(1, 1 + int(da_num_lbls)))\n",
        "        )\n",
        "\n",
        "        da_lbl_img, da_num_lbls = dask_ndmeasure.label(\n",
        "            (\n",
        "                ((da_area_lbls >= area_min_threshold)[(slice(None),) + da_lbl_img.ndim * (None,)] * da_lbl_img) > 0\n",
        "            ).sum(axis=0)\n",
        "        )\n",
        "\n",
        "        da_lbl_img, da_num_lbls = executor.persist([da_lbl_img, da_num_lbls])\n",
        "\n",
        "        da_result = []\n",
        "        for i in irange(1, 1 + int(da_num_lbls)):\n",
        "            da_result.append((da_lbl_img == i)[None])\n",
        "        da_result = da.concatenate(da_result)\n",
        "\n",
        "        with open_zarr(data_basename + postfix_post + zarr_ext, \"w\") as f2:\n",
        "            result = f2.create_group(\"rois\").create_dataset(\n",
        "                \"mask\",\n",
        "                shape=da_result.shape,\n",
        "                dtype=da_result.dtype,\n",
        "                chunks=True\n",
        "            )\n",
        "            da_result = da_result.rechunk(result.chunks)\n",
        "            status = executor.compute(da.store(da_result, result, lock=False, compute=False))\n",
        "            dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_post + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_post + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_post + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_post + zarr_ext, \"rois/mask\")[...][...].astype(np.uint8)\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=result_image_stack.min(),\n",
        "        vmax=result_image_stack.max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Threshold data\n",
        "\n",
        "* `block_frames` (`int`): number of frames to work with in each full frame block (run in parallel).\n",
        "* `block_space` (`int`): extent of each spatial dimension for each block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "block_frames = 40\n",
        "block_space = 300\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_thrd + zarr_ext)\n",
        "io_remove(data_basename + postfix_thrd + h5_ext)\n",
        "\n",
        "with open_zarr(data_basename + postfix_wt + zarr_ext, \"r\") as f:\n",
        "    with open_zarr(data_basename + postfix_post + zarr_ext, \"r\") as f2:\n",
        "        with get_executor(client) as executor:\n",
        "            # Load and prep data for computation.\n",
        "            imgs = f[\"images\"]\n",
        "            da_imgs = da.from_array(\n",
        "                imgs, chunks=(block_frames,) + (imgs.ndim - 1) * (block_space,)\n",
        "            )\n",
        "            msks = f2[\"rois/mask\"]\n",
        "            da_msks = da.from_array(\n",
        "                msks, chunks=(block_frames,) + (imgs.ndim - 1) * (block_space,)\n",
        "            )\n",
        "\n",
        "            da_result = da_imgs * da_msks.max(axis=0, keepdims=True).astype(da_imgs.dtype)\n",
        "\n",
        "            # Store data\n",
        "            with open_zarr(data_basename + postfix_thrd + zarr_ext, \"w\") as f2:\n",
        "                result = f2.create_dataset(\n",
        "                    \"images\",\n",
        "                    shape=da_result.shape,\n",
        "                    dtype=da_result.dtype,\n",
        "                    chunks=True\n",
        "                )\n",
        "                da_result = da_result.rechunk(result.chunks)\n",
        "                status = executor.compute(da.store(da_result, result, lock=False, compute=False))\n",
        "                dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "\n",
        "zip_zarr(data_basename + postfix_thrd + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_thrd + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_thrd + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_thrd + zarr_ext, \"images\")[...][...]\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=result_image_stack.min(),\n",
        "        vmax=result_image_stack.max()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ROI and trace extraction\n",
        "\n",
        "* `block_frames` (`int`): number of frames to work with in each block (run in parallel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "block_frames = 100\n",
        "\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_rois + zarr_ext)\n",
        "io_remove(data_basename + postfix_rois + h5_ext)\n",
        "\n",
        "with open_zarr(data_basename + postfix_rois + zarr_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_post + zarr_ext, \"r\") as f1:\n",
        "        f2[\"masks\"] = f1[\"rois/mask\"]\n",
        "\n",
        "    mskimg = f2[\"masks\"]\n",
        "    mskimg_j = f2.create_dataset(\"masks_j\", shape=mskimg.shape, dtype=numpy.uint8, chunks=True)\n",
        "    par_norm_layer(num_frames=block_frames)(mskimg, out=mskimg_j)\n",
        "\n",
        "    lblimg = label_mask_stack(mskimg, np.uint64)\n",
        "    f2[\"labels\"] = lblimg\n",
        "    f2[\"labels_j\"] = lblimg.astype(np.uint16)\n",
        "    lblimg = f2[\"labels\"]\n",
        "\n",
        "zip_zarr(data_basename + postfix_rois + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_rois + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_rois + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "# Somehow we can't overwrite the file in the container so this is needed.\n",
        "io_remove(data_basename + postfix_traces + zarr_ext)\n",
        "io_remove(data_basename + postfix_traces + h5_ext)\n",
        "\n",
        "with open_zarr(data_basename + postfix_f_f0 + zarr_ext, \"r\") as fh_f_f0:\n",
        "    with open_zarr(data_basename + postfix_rois + zarr_ext, \"r\") as fh_rois:\n",
        "        with get_executor(client) as executor:\n",
        "            # Load and prep data for computation.\n",
        "            images = fh_f_f0[\"images\"]\n",
        "            da_images = da.from_array(\n",
        "                images, chunks=(block_frames,) + images.shape[1:]\n",
        "            )\n",
        "            masks = fh_rois[\"masks\"]\n",
        "            da_masks = da.from_array(\n",
        "                masks, chunks=(block_frames,) + masks.shape[1:]\n",
        "            )\n",
        "\n",
        "            da_result = compute_traces(da_images, da_masks)\n",
        "\n",
        "            # Store taces\n",
        "            with open_zarr(data_basename + postfix_traces + zarr_ext, \"w\") as fh_traces:\n",
        "                result = fh_traces.create_dataset(\n",
        "                    \"traces\",\n",
        "                    shape=da_result.shape,\n",
        "                    dtype=da_result.dtype,\n",
        "                    chunks=True\n",
        "                )\n",
        "                da_result = da_result.rechunk(result.chunks)\n",
        "                status = executor.compute(da.store(da_result, result, lock=False, compute=False))\n",
        "                dask.distributed.progress(status, notebook=False)\n",
        "\n",
        "zip_zarr(data_basename + postfix_traces + zarr_ext)\n",
        "\n",
        "with h5py.File(data_basename + postfix_traces + h5_ext, \"w\") as f2:\n",
        "    with open_zarr(data_basename + postfix_traces + zarr_ext, \"r\") as f1:\n",
        "        zarr_to_hdf5(f1, f2)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    result_image_stack = LazyZarrDataset(data_basename + postfix_f_f0 + zarr_ext, \"images\")\n",
        "    lblimg = LazyZarrDataset(data_basename + postfix_rois + zarr_ext, \"labels\")\n",
        "\n",
        "    mplsv = plt.figure(FigureClass=MPLViewer)\n",
        "    mplsv.set_images(\n",
        "        result_image_stack,\n",
        "        vmin=par_compute_min_projection(num_frames=block_frames)(result_image_stack).min(),\n",
        "        vmax=par_compute_max_projection(num_frames=block_frames)(result_image_stack).max()\n",
        "    )\n",
        "\n",
        "    lblimg = lblimg[...][...]\n",
        "    lblimg_msk = numpy.ma.masked_array(lblimg, mask=(lblimg==0))\n",
        "\n",
        "    mplsv.viewer.matshow(lblimg_msk, alpha=0.3, cmap=mpl.cm.jet)\n",
        "\n",
        "\n",
        "mskimg = None\n",
        "mskimg_j = None\n",
        "lblimg = None\n",
        "traces = None\n",
        "traces_j = None\n",
        "\n",
        "del mskimg\n",
        "del mskimg_j\n",
        "del lblimg\n",
        "del traces\n",
        "del traces_j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End of workflow. Shutdown cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nanshe_workflow.par import cleanup_cluster_files\n",
        "\n",
        "ipypar_prof = \"sge\"\n",
        "\n",
        "from sys import executable as PYTHON\n",
        "!$PYTHON -m ipyparallel.apps.ipclusterapp stop --profile=$ipypar_prof\n",
        "del PYTHON\n",
        "\n",
        "cleanup_cluster_files(ipypar_prof)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare interactive projection graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import textwrap\n",
        "import zlib\n",
        "\n",
        "import numpy\n",
        "import numpy as np\n",
        "\n",
        "import scipy\n",
        "import scipy as sp\n",
        "\n",
        "import scipy.ndimage\n",
        "import scipy.ndimage as spim\n",
        "\n",
        "import h5py\n",
        "import h5py as hp\n",
        "\n",
        "import bokeh.plotting\n",
        "import bokeh.plotting as bp\n",
        "\n",
        "import bokeh.io\n",
        "import bokeh.io as bio\n",
        "\n",
        "import bokeh.embed\n",
        "import bokeh.embed as be\n",
        "\n",
        "from bokeh.models.mappers import LinearColorMapper\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.cm\n",
        "\n",
        "from matplotlib.colors import ColorConverter\n",
        "from matplotlib.cm import gist_rainbow\n",
        "\n",
        "import webcolors\n",
        "\n",
        "from bokeh.models import CustomJS, ColumnDataSource, HoverTool\n",
        "from bokeh.models.layouts import Row\n",
        "\n",
        "from builtins import (\n",
        "    map as imap,\n",
        "    range as irange\n",
        ")\n",
        "\n",
        "from past.builtins import basestring\n",
        "\n",
        "import nanshe\n",
        "\n",
        "import xnumpy\n",
        "import xnumpy.core\n",
        "from xnumpy.core import expand\n",
        "\n",
        "import nanshe_workflow\n",
        "from nanshe_workflow.data import io_remove, open_zarr\n",
        "from nanshe_workflow.vis import get_rgb_array, get_rgba_array, get_all_greys, masks_to_contours_2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open_zarr(data_basename + postfix_rois + zarr_ext, \"r\") as f:\n",
        "    mskimg = f[\"masks\"][...]\n",
        "\n",
        "with open_zarr(data_basename + postfix_traces + zarr_ext, \"r\") as f:\n",
        "    traces = f[\"traces\"][...]\n",
        "\n",
        "with open_zarr(data_basename + postfix_proj + zarr_ext, \"r\") as f:\n",
        "    imgproj_mean = f[\"mean\"][...]\n",
        "    imgproj_max = f[\"max\"][...]\n",
        "    imgproj_std = f[\"std\"][...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result visualization\n",
        "\n",
        "* `proj_img` (`str` or `list` of `str`): which projection or projections to plot (e.g. \"max\", \"mean\", \"std\").\n",
        "* `block_size` (`int`): size of each point on any dimension in the image in terms of pixels.\n",
        "* `roi_alpha` (`float`): transparency of the ROIs in a range of [0.0, 1.0].\n",
        "* `roi_border_width` (`int`): width of the line border on each ROI.\n",
        "\n",
        "<br>\n",
        "* `trace_plot_width` (`int`): width of the trace plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "proj_img = \"std\"\n",
        "block_size = 1\n",
        "roi_alpha = 0.3\n",
        "roi_border_width = 3\n",
        "trace_plot_width = 500\n",
        "\n",
        "\n",
        "bio.curdoc().clear()\n",
        "\n",
        "grey_range = get_all_greys()\n",
        "grey_cm = LinearColorMapper(grey_range)\n",
        "\n",
        "colors_rgb = get_rgb_array(len(mskimg))\n",
        "colors_rgb = colors_rgb.tolist()\n",
        "colors_rgb = list(imap(webcolors.rgb_to_hex, colors_rgb))\n",
        "\n",
        "mskctr_pts_y, mskctr_pts_x = masks_to_contours_2d(mskimg)\n",
        "\n",
        "mskctr_pts_dtype = np.min_scalar_type(max(mskimg.shape[1:]) - 1)\n",
        "mskctr_pts_y = [np.array(_, dtype=mskctr_pts_dtype) for _ in mskctr_pts_y]\n",
        "mskctr_pts_x = [np.array(_, dtype=mskctr_pts_dtype) for _ in mskctr_pts_x]\n",
        "\n",
        "mskctr_srcs = ColumnDataSource(data=dict(x=mskctr_pts_x, y=mskctr_pts_y, color=colors_rgb))\n",
        "\n",
        "\n",
        "if isinstance(proj_img, basestring):\n",
        "    proj_img = [proj_img]\n",
        "else:\n",
        "    proj_img = list(proj_img)\n",
        "\n",
        "\n",
        "proj_plot_width = block_size*mskimg.shape[2]\n",
        "proj_plot_height = block_size*mskimg.shape[1]\n",
        "plot_projs = []\n",
        "\n",
        "if \"max\" in proj_img:\n",
        "    plot_max = bp.Figure(plot_width=proj_plot_width, plot_height=proj_plot_height,\n",
        "                         x_range=[0, mskimg.shape[2]], y_range=[mskimg.shape[1], 0],\n",
        "                         tools=[\"tap\", \"pan\", \"box_zoom\", \"resize\", \"wheel_zoom\", \"save\", \"reset\"],\n",
        "                         title=\"Max Projection with ROIs\", border_fill_color=\"black\")\n",
        "    plot_max.image(image=[numpy.flipud(imgproj_max)], x=[0], y=[mskimg.shape[1]],\n",
        "                   dw=[imgproj_max.shape[1]], dh=[imgproj_max.shape[0]], color_mapper=grey_cm)\n",
        "    plot_max.patches('x', 'y', source=mskctr_srcs, alpha=roi_alpha, line_width=roi_border_width, color=\"color\")\n",
        "\n",
        "    plot_max.outline_line_color = \"white\"\n",
        "    for i in irange(len(plot_max.axis)):\n",
        "        plot_max.axis[i].axis_line_color = \"white\"\n",
        "\n",
        "    plot_projs.append(plot_max)\n",
        "\n",
        "\n",
        "if \"mean\" in proj_img:\n",
        "    plot_mean = bp.Figure(plot_width=proj_plot_width, plot_height=proj_plot_height,\n",
        "                         x_range=[0, mskimg.shape[2]], y_range=[mskimg.shape[1], 0],\n",
        "                         tools=[\"tap\", \"pan\", \"box_zoom\", \"resize\", \"wheel_zoom\", \"save\", \"reset\"],\n",
        "                         title=\"Mean Projection with ROIs\", border_fill_color=\"black\")\n",
        "    plot_mean.image(image=[numpy.flipud(imgproj_mean)], x=[0], y=[mskimg.shape[1]],\n",
        "                   dw=[mskimg.shape[2]], dh=[mskimg.shape[1]], color_mapper=grey_cm)\n",
        "    plot_mean.patches('x', 'y', source=mskctr_srcs, alpha=roi_alpha, line_width=roi_border_width, color=\"color\")\n",
        "\n",
        "    plot_mean.outline_line_color = \"white\"\n",
        "    for i in irange(len(plot_mean.axis)):\n",
        "        plot_mean.axis[i].axis_line_color = \"white\"\n",
        "\n",
        "    plot_projs.append(plot_mean)\n",
        "\n",
        "\n",
        "if \"std\" in proj_img:\n",
        "    plot_std = bp.Figure(plot_width=proj_plot_width, plot_height=proj_plot_height,\n",
        "                         x_range=[0, mskimg.shape[2]], y_range=[mskimg.shape[1], 0],\n",
        "                         tools=[\"tap\", \"pan\", \"box_zoom\", \"resize\", \"wheel_zoom\", \"save\", \"reset\"],\n",
        "                         title=\"Std Dev Projection with ROIs\", border_fill_color=\"black\")\n",
        "    plot_std.image(image=[numpy.flipud(imgproj_std)], x=[0], y=[mskimg.shape[1]],\n",
        "                   dw=[mskimg.shape[2]], dh=[mskimg.shape[1]], color_mapper=grey_cm)\n",
        "    plot_std.patches('x', 'y', source=mskctr_srcs, alpha=roi_alpha, line_width=roi_border_width, color=\"color\")\n",
        "\n",
        "    plot_std.outline_line_color = \"white\"\n",
        "    for i in irange(len(plot_std.axis)):\n",
        "        plot_std.axis[i].axis_line_color = \"white\"\n",
        "\n",
        "    plot_projs.append(plot_std)\n",
        "\n",
        "\n",
        "all_tr_dtype_srcs = ColumnDataSource(data=dict(traces_dtype=traces.dtype.type(0)[None]))\n",
        "all_tr_shape_srcs = ColumnDataSource(data=dict(traces_shape=traces.shape))\n",
        "all_tr_srcs = ColumnDataSource(data=dict(\n",
        "    traces=numpy.frombuffer(\n",
        "        zlib.compress(traces.tobytes()),\n",
        "        dtype=np.uint8\n",
        "    )\n",
        "))\n",
        "tr_srcs = ColumnDataSource(data=dict(times_sel=[], traces_sel=[], colors_sel=[]))\n",
        "plot_tr = bp.Figure(plot_width=trace_plot_width, plot_height=proj_plot_height,\n",
        "                    x_range=(0.0, float(traces.shape[1])), y_range=(float(traces.min()), float(traces.max())),\n",
        "                    tools=[\"pan\", \"box_zoom\", \"resize\", \"wheel_zoom\", \"save\", \"reset\"], title=\"ROI traces\",\n",
        "                    background_fill_color=\"black\", border_fill_color=\"black\")\n",
        "plot_tr.multi_line(\"times_sel\", \"traces_sel\", source=tr_srcs, color=\"colors_sel\")\n",
        "\n",
        "plot_tr.outline_line_color = \"white\"\n",
        "for i in irange(len(plot_tr.axis)):\n",
        "    plot_tr.axis[i].axis_line_color = \"white\"\n",
        "\n",
        "plot_projs.append(plot_tr)\n",
        "\n",
        "\n",
        "mskctr_srcs.callback = CustomJS(\n",
        "    args=dict(\n",
        "        all_tr_dtype_srcs=all_tr_dtype_srcs,\n",
        "        all_tr_shape_srcs=all_tr_shape_srcs,\n",
        "        all_tr_srcs=all_tr_srcs,\n",
        "        tr_srcs=tr_srcs\n",
        "    ), code=\"\"\"\n",
        "    var range = function(n){ return Array.from(Array(n).keys()); };\n",
        "\n",
        "    var traces_not_decoded = (all_tr_dtype_srcs.get('data')['traces_dtype'] == 0);\n",
        "    var traces_dtype = all_tr_dtype_srcs.get('data')['traces_dtype'].constructor;\n",
        "    var traces_shape = all_tr_shape_srcs.get('data')['traces_shape'];\n",
        "    var trace_len = traces_shape[1];\n",
        "    var traces = all_tr_srcs.get('data')['traces'];\n",
        "    if (traces_not_decoded) {\n",
        "        traces = window.pako.inflate(traces);\n",
        "        traces = new traces_dtype(traces.buffer);\n",
        "        all_tr_srcs.get('data')['traces'] = traces;\n",
        "        all_tr_dtype_srcs.get('data')['traces_dtype'] = 1;\n",
        "    }\n",
        "\n",
        "    var inds = cb_obj.get('selected')['1d'].indices;\n",
        "    var colors = cb_obj.get('data')['color'];\n",
        "    var selected = tr_srcs.get('data');\n",
        "\n",
        "    var times = range(trace_len);\n",
        "\n",
        "    selected['times_sel'] = [];\n",
        "    selected['traces_sel'] = [];\n",
        "    selected['colors_sel'] = [];\n",
        "\n",
        "    for (i = 0; i < inds.length; i++) {\n",
        "        var inds_i = inds[i];\n",
        "        var trace_i = traces.slice(trace_len*inds_i, trace_len*(inds_i+1));\n",
        "        var color_i = colors[inds_i];\n",
        "\n",
        "        selected['times_sel'].push(times);\n",
        "        selected['traces_sel'].push(trace_i);\n",
        "        selected['colors_sel'].push(color_i);\n",
        "    }\n",
        "\n",
        "    tr_srcs.trigger('change');\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "plot_group = Row(*plot_projs)\n",
        "\n",
        "\n",
        "# Clear out the old HTML file before writing a new one.\n",
        "io_remove(data_basename + postfix_html + html_ext)\n",
        "\n",
        "\n",
        "def indent(text, spaces):\n",
        "    spaces = \" \" * int(spaces)\n",
        "    return \"\\n\".join(imap(lambda l: spaces + l, text.splitlines()))\n",
        "\n",
        "def write_html(filename, title, div, script, cdn):\n",
        "    html_tmplt = textwrap.dedent(u\"\"\"\\\n",
        "        <html lang=\"en\">\n",
        "            <head>\n",
        "                <meta charset=\"utf-8\">\n",
        "                <title>{title}</title>\n",
        "                {cdn}\n",
        "                <style>\n",
        "                  html {{\n",
        "                    width: 100%;\n",
        "                    height: 100%;\n",
        "                  }}\n",
        "                  body {{\n",
        "                    width: 90%;\n",
        "                    height: 100%;\n",
        "                    margin: auto;\n",
        "                    background-color: black;\n",
        "                  }}\n",
        "                </style>\n",
        "            </head>\n",
        "            <body>\n",
        "                {div}\n",
        "                {script}\n",
        "            </body>\n",
        "        </html>\n",
        "    \"\"\")\n",
        "\n",
        "    html_cont = html_tmplt.format(\n",
        "        title=title,\n",
        "        div=indent(div, 8),\n",
        "        script=indent(script, 8),\n",
        "        cdn=indent(cdn, 8),\n",
        "    )\n",
        "\n",
        "    with io.open(filename, \"w\") as fh:\n",
        "        fh.write(html_cont)\n",
        "\n",
        "script, div = be.components(plot_group)\n",
        "cdn = bokeh.resources.CDN.render() + \"\\n\"\n",
        "cdn += \"\"\"\n",
        "<script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/pako/1.0.4/pako_inflate.min.js\"></script>\n",
        "\"\"\"\n",
        "cdn += \"\\n\"\n",
        "\n",
        "write_html(data_basename + postfix_html + html_ext, data_basename + postfix_html, div, script, cdn)\n",
        "\n",
        "\n",
        "if __IPYTHON__:\n",
        "    from IPython.display import display, IFrame\n",
        "    display(IFrame(data_basename + postfix_html + html_ext, \"100%\", 1.05*proj_plot_height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test teardown. Ignore warnings during production runs.\n",
        "\n",
        "%run ./teardown_tests.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "widgets": {
      "state": {},
      "version": "1.2.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
